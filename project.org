* NN SGX
Running the dense part of CNNs inside the trusted enclave to reduce leakage and protect against model stealing.
We hope to make this as robust against model stealing as online oracles.

** People
RBO, CPA, ASC

** Project Outline
*** First prototype [14/14]
**** DONE Create working environment for Testing and Development
     CLOSED: [2019-08-20 Tue 19:55]
[2019-08-14 Wed]
The project unfortunately only runs on Ubuntu 18.04 and some other OSes, and not on my own machine...
I'm currently developing inside a VM.

The project currently consists of a minimal Makefile for a single enclave and app that runs in simulation mode.
Adding functions to the enclave requires their definition in the header file, and implementation, as for regular C code.
Additionally, it requires addition to the ~trusted~ block in [[file:Enclave/Enclave.edl::trusted%20{][Enclave/Enclave.edl]], with publicly accessible functions having the ~public~ keyword added to them.

In the app, the functions are then called with a slightly different signature.
The return value of enclave functions is always an sgx_status, and the ~global_eid~ is added to the parameters.
Return values of the function are set by the SGX wrapper through pointers, C-style.

**** DONE Extract Weights from neural net
     CLOSED: [2019-09-16 Mon 16:50]
     I can store the weights of a model in HDF5 format, and then load that using their C++ API.
     Then I could either load the weights from inside the enclave (which gives us no benefit at all), or hardcode them.

     I need the activation functions anyway, so hardcoding is probably the way to go.

     [2019-09-03 Tue 09:40]
     FUCK TENSORFLOW
     The C++ API requires building from source, and that requires bazel, and then everything together is a massive house of cards where the cards randomly self ignite, and then the example doesn't compile and nothing works.
     That seems like an unreasonable amount of super annoying work, which I don't want to do.

     So, instead I will try to create the NN in python, and create cython bindings for my C++ code.
     This probably means I will have to write a wrapper app for the enclave code, and maybe later make calls to the enclave directly.

     [2019-09-04 Wed 14:24]
     Does it have to be Tensorflow?
     PyTorch seems to do the same thing but nicer, because it gives us more granular access to the underlying matrices.

     [2019-09-16 Mon 14:39]
     I moved the code for executing the dense layers to their own function in the ~Net~ class.
     Now I can load the weights from the state_dict and then do the computation using custom built functions for the matrix multiplication.
     This should serve as a good starting point for moving that first to C and then to the SGX enclave.
     
     [2019-09-16 Mon 15:30]
     Finished moving the dense NN calculation to plain numpy.
     It's actually fairly simple, I just need to remember to transpose the weight matrix.
     Here's the source code for it:
#+BEGIN_SRC python
    def dense(self, x):
        state = self.state_dict()
        # breakpoint()
        w1 = state['fc1.weight'].detach().numpy().T
        b1 = state['fc1.bias'].detach().numpy()
        x = x.detach().numpy()
        tmp = np.matmul(x, w1) + b1
        x = np.maximum(tmp, 0)

        w2 = state['fc2.weight'].detach().numpy().T
        b2 = state['fc2.bias'].detach().numpy()
        tmp = np.matmul(x, w2) + b2
        x = np.maximum(tmp, 0)
        return torch.Tensor(x)
#+END_SRC
     
**** DONE Understand biases in Pytorch NNs
     CLOSED: [2019-09-16 Mon 16:50]
I need to understand how biases are used in pytorch, so I can correctly model that behaviour.

     [2019-09-16 Mon 16:50]
They are simply added after the input is multiplied with the weight.
Then the sum of the multiplication result and the biases is run through the nonlinearity.

     [2019-09-25 Wed 10:54]
The reason why I was so confused by biases in Pytorch is the fact that multiple inputs in a batch are ~hstacked~ together into a single matrix.
This means the computation the tensor does internally is more than simple matrix multiplication.
In order to do this correctly manually, one has to iterate over every column and compute the matrix multiplication for that column alone.
Then the dimensions of the bias vector also fit again.

**** DONE Write a script that generates C arrays from pytorch weight matrices
     CLOSED: [2019-09-16 Mon 16:51]
     Because the weights should be hardcoded into the enclave (this avoids decryption during loading for now), I need a script to extract weights from a ~.pt~ file and generate C array definitions from it.

     See [[./python/torch/gen_headers.py][this script]].

**** DONE Choose test network for first prototype
     CLOSED: [2019-09-05 Thu 10:54]
     Just use MNIST CNN, I have working code for torch

**** DONE Write naive matrix multiplication code in C [3/3]
     CLOSED: [2019-09-18 Wed 10:13]
Just simple stuff, including testing
- [X] matrix matrix multiplication
- [X] matrix matrix addition
- [X] nonlinearity

**** DONE Compare results of torch, numpy and my C code
     CLOSED: [2019-09-19 Thu 10:29]
There seem to be some differences in results that can't entirely be blamed on rounding errors.
The differences are currently in the range of e-7, which is more than numpy uses for its ~np.allclose~ function (e-8).
I will build a small network and then compare all three methods of computation, to see what is happening and where it starts to diverge.

[2019-09-19 Thu 10:26]
Finished evaluation, see [[file:test_correctness.py::fc%20=%20nn.Linear(IN_FEATURES,%20OUT_FEATURES,%20bias=False)][test_correctness.py]].
Even though the differences are around e-6, they seem to be normal distributed, and the max/min increases with increased matrix size (which strengthens my belief in the normal distribution).
I don't think it is a problem, and if it becomes one I already have a test script set up for evaluation.

**** DONE Move the fully connected layers to naive matrix multiplication in C
     CLOSED: [2019-09-19 Thu 10:29]
In the forward function of the network, instead of invoking ~nn.Linear~ I can call a C function.
This means that pytorch doesn't know how to backpropagate, but it doesn't need to anymore.

[2019-09-18 Wed 17:10]
The numpy and C variants do give slightly different results than the pure pytorch variant (in the e-7 range).
Jakob thinks this is more than just rounding errors, so I should check that out.
See [[*Compare results of torch, numpy and my C code][this TODO]]

**** DONE Combine C functions into one ~dense~ function
     CLOSED: [2019-09-19 Thu 12:06]
     This function can then be moved to the enclave, otherwise it leaks intermediate values

**** DONE Add compile_commands
     CLOSED: [2019-09-19 Thu 14:21]
Added a single recursive Makefile, which can be wrapped using bear

**** DONE Check the details of parameter passing into enclaves
     CLOSED: [2019-09-23 Mon 11:34]
Simply passing input pointers to the matrices doesn't work, so I will need to test this with some smaller examples
[2019-09-23 Mon 11:33]
Adding a [in, count=s] parameter for every array tells the SGX autogenerated wrapper how large the array should be, and it is then copied to enclave memory

**** DONE Do naive matrix multiplication inside the SGX enclave
     CLOSED: [2019-09-23 Mon 11:34]
Move the aforementioned code function to the enclave.
With a well enough defined interface this shouldn't be too much work

**** DONE Feed result back into pytorch, or calculate softmax
     CLOSED: [2019-09-19 Thu 14:25]
As we want this to be an oracle, I should execute the "softmax" (just taking the maximum)

[2019-09-19 Thu 14:21]
matutil_dense() already only returns the label

**** DONE Find a way to actually call the Enclave from PyTorch
     CLOSED: [2019-09-23 Mon 14:55]
Currently the enclave is not called from pytorch
The wrapper is called correctly, and then no error happens, but the enclave is not called.

[2019-09-23 Mon 14:22]
I had not initialized the enclave, and did not really output any errors in the enclave wrapper.
So yeah, this one's on me.

[2019-09-23 Mon 14:40]
Added some error handling to pymatutil.

*** Improvements for paper [1/6]
**** TODO Test on actual hardware [1/2]
We want to test this on some actual hardware.
For a representative evaluation we should use one consumer grade, and one server grade CPU.
According to [[https://software.intel.com/en-us/forums/intel-software-guard-extensions-intel-sgx/topic/606636][this forum answer]] Intel SGX was introduced with Skylake generation CPUs, so any 6xxx processor will be fine.

***** DONE Write an e-mail to Manuel
      CLOSED: [2019-09-25 Wed 13:31]
***** WATING Set up the machine
I have a [[file:setup_sgx_machine.sh][setup script]] for Ubuntu 19.04, which I tested on VMs.

**** DONE Find out how model architectures are stored
     CLOSED: [2019-09-25 Wed 10:49]
We want to be able to specify a cut similar to current SOTA architecture visualization tools.
***** PyTorch
PyTorch uses the python classes for model definition, usually only storing the state_dict.
However, it is also possible to store the entire *module* where the model is defined, using Python's [[https://docs.python.org/3/library/pickle.html][pickle]] module.
Internally this does the same thing as keeping the file containing the python class and loading the state_dict, as it stores that file as well.
This means it is dependent on the containing folder structure, which might lead to some very weird errors.

PyTorch has no innate visualization tool, it instead is directed more at a programmer's view of things, as it allows for debugging the actual code of the ~forward~ function.
What one can do is export the model to [[ONNX][ONNX]] format, and then visualize it using something like [[https://github.com/PaddlePaddle/VisualDL][VisualDL]].

For our separation we could create some macro, wrapper, whatever that is then easily plugged into the definition of ~forward~.

***** Tensorflow
For Tensorflow, everything is in the session.
The graph that describes the model, the state of layers, optimizers etc.
The problem is that this way I'm not sure if my code would even work, as this would mean rebuilding everything so it works with Tensorflow.

Tensorflow visualizes its graphs using Tensorboard, and gets the info for that from the log directory.
A link to this is [[https://www.tensorflow.org/guide/graph_viz][here]].
I'm not sure how easy we can use Tensorboard for input, as Tensorflow in general is hard to edit.

***** Open Neural Network Exchange - <<ONNX>>
[[https://github.com/onnx/onnx][ONNX]] is meant to be a framework independent way of specifying models, complete with data types and operators.
There are ways to *export* to ONNX format from most common tools, but not many have a way of *importing*.
This makes it very difficult to use for our purposes, as my code would then still be framework dependent, just using an independent way to specify the model.

ONNX does have its own runtime, and I could try and move dense parts of that to the SGX enclave, but that would make comparison harder for the (I assume) not super broadly adopted ONNX runtime.

***** Keras
Keras tries to be a human readable extension sitting on top of Tensorflow, Theano etc.
The layers of Keras (and therefore any custom layers) can be written in pure python, and I think they can then also call C functions.

Storing models in Keras saves the architecture as well as the weights.
Anything that also contains weights is stored as an [[https://en.wikipedia.org/wiki/Hierarchical_Data_Format][HDF5]] file.
It's also possible to store the architecture alone as a json file.

Keras can do rudimentary visualization by using pydot, which uses [[https://github.com/pydot/pydot][pydot]] to interface with [[http://www.graphviz.org/][graphviz]].
The function for this is ~plot_model~ in ~keras.utils~.
Alternatively Keras offers a ~model.summary()~ function which prints a summary of all the layers.

For these reasons, *Keras* seems to be the best choice, offering the versatility and customizability we need along with nice tooling for training, storing and loading, as well as visualization.

**** TODO Design a splitting functionality for Keras [2/3]
RBO wants to have a good way of specifying split position and visualization of it before we try and run the code.
The best way to do this is writing a Keras layer container (similar to Sequential).
This way we have control over how the underlying layers function.

***** DONE Fix plotting for composed Keras models
      CLOSED: [2019-09-25 Wed 15:56]
Keras models can be composed from each other.
There is a concat function, but one model can actually just contain another model.
For both the ~model.summary()~ function and the ~plot~ function however, the contained model is not summarized recursively.
It is just listed as ~sequential_2~.
This will be the first step.

[2019-09-25 Wed 15:55]
~plot_model~ has a parameter for expanding submodules, called ~expand_nested~.
Setting this to true gives output like in [[file:python/mnist_cnn.png][this image]].

***** DONE Finish test code for MNIST in Keras
      CLOSED: [2019-09-25 Wed 16:25]
In order to evaluate I need fully working code for MNIST

***** TODO Write extension to Keras Sequential [2/10]
We want to have a container similar to Keras's ~Sequential~ model that can also generate C code for the contained layers.
It should be possible to iterate over all contained layers and store the ~output_shape~ along the way.
Then I also need to store the ~input_shape~ of the first layer and the weights and biases of all layers.
Getting the activation functions could be a bit trickier.
Here it is probably best to either call the C functions like the Keras names, or introduce a mapping.
I'm not sure right now if I can actually get the name of the activation function or not, which could increase difficulty.

[2019-09-25 Wed 18:32]
When executing a Keras Sequential (e.g. for ~predict()~) at the execution at one point starts calling backend functions.
This is something I can't really trace, and try and emulate this with my extension to ~Sequential~.
Due to this it's smarter to build a function that takes a sequential model and builds the C code from that.
This could then be wrapped in a custom Keras layer that calls the C code.
I would first however have to test whether calling C code from a Keras layer works at all.

[2019-09-26 Thu 10:52]
I took a debugging dive into the ~model.predict()~ function.
It's fairly easy to follow until Keras hands off execution to the Tensorflow backend.
The model is collapsed into a single function, which makes extensive use of the TF session model (ugh...).
This function is then wrapped into a Keras ~Function~ object, which is also a TF ~GraphExecutionFunction~ object when using the TF backend.

****** DONE Test control flow for custom Keras layers
       CLOSED: [2019-09-26 Thu 12:02]
[2019-09-26 Thu 11:54]
Keras only sits on top of whatever backend it uses.
This means that any custom Keras layer really only generates TF tensors.
The contents of their [[https://keras.io/layers/writing-your-own-keras-layers/][~call()~]] function are really only called once, where they are converted to TF Graphexecution functions.
Also, the ~x~ input is a backend tensor, so there is not really any way to call my C function.
For that I would need the actual values, which they don't have at that point, because they are only placeholders...

The real problem is the inherent lazyness of TF 1.14.
*HOWEVER*, there is a way to add eager functions to it, by wrapping them with [[https://www.tensorflow.org/api_docs/python/tf/py_function][~tf.py_func~]].
This is a tensor wrapping a regular C function.
With this I can wrap my ~pymatutil~ function in a regular python function, taking numpy arrays as input.

Keras doesn't really have an equivalent.
I'm currently not sure whether it's smarter to switch to TF in general or just use the TF function in Keras.
I will check how easy it is to get weights out of TF NNs.

With TF 2.0 (which is currently available as a beta) eager execution is the default, so this would make things a lot easier.

[2019-09-26 Thu 16:00]
So I switched to TF 2.0.0 rc2, because the eagerness seems to be exactly what I want.
Unfortunately, during training it still switches to lazy execution, I guess for performance.
However, once everything is done the environment is in eager mode again.
I guess they never meant eager execution to work during full blown execution of the network.
*HOWEVER*, this still helps me to write my code, as I can use the Keras bundled with TF, and use [[https://www.tensorflow.org/api_docs/python/tf/py_function][~tf.py_func~]] to call my code.
Will test tomorrow.

****** TODO Usue ~tf.py_func~ to call C code for testing
It should probably be a simple operation like multiplying with 2.

****** CANCELED Test calling C Code from a custom Keras layer
       CLOSED: [2019-09-26 Thu 11:57]
Does not make sense, see [[*Test control flow for custom Keras layers][this point]], lazyness is super annoying.

****** TODO Extend Keras ~Sequential~
****** TODO Store input shape of the first layer
****** TODO Store output shapes of all layers
****** TODO Store weights of layers
****** TODO Store biases if applicable
****** TODO Generate matrix multiplications
****** TODO Generate activation functions
**** TODO Build custom Keras layer for C code execution
After the C code is generated, the custom Sequential model should either call that instead of the underlying layers, or replace itself with a new layer.
That new layer would then either call the Enclave or native C code.

**** TODO Test the splitting with several popular architectures
We should test the splitting with several common architectures.
RBO also said something about splitting according to some metrics (e.g. GPU memory/utilization), which I'm not too sure about how he meant it.
We could try and split before the first dense layer, I should try and see if there are other layer types used for actual classification.

**** TODO Automate memory layout inside SGX
We might have to do some memory magic because otherwise we might run out of memory inside the SGX.
The first prototype can do this explicitly for the chosen network, but for publishing we should do this automatically.

Alternative we could also generate a fixed function from sparse matrix multiplication.
This would mean that we go through the output cell by cell, calculating all immediate steps in a row.
Using this would throw away any shared results, and be much slower.
However, this could help us avoid memory issues.

[2019-09-25 Wed 10:56]
This has to wait until we are testing on actual hardware.
For the simulation mode we are currently running, memory doesn't seem to be an issue.
I'm currently specifying intermediate matrices after every ~fc~ layer of the network (see [[file:lib/sgx/Enclave/Enclave.cpp][the enclave code]]), without any issues.

This either means we are not as memory constrained as we thought, or the simulation simply has a larger amount of memory than the actual enclave would have.

*** Future Work [0/1]
**** TODO Integrate with the framework API
Rainer said it would be nice to integrate the SGX with the tensorflow API (or pytorch, whatever)

** Unforeseen events
*** TODO Figure out how many bits the enclave uses for floats
This could cause some weird results and incompatibilities.
It's also not perfectly clear if the enclave even supports floats.
