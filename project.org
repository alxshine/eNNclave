* NN SGX
Running the dense part of CNNs inside the trusted enclave to reduce leakage and protect against model stealing.
We hope to make this as robust against model stealing as online oracles.

** People
RBO, CPA, ASC

** Project Outline
*** First prototype [14/14]
**** DONE Create working environment for Testing and Development
     CLOSED: [2019-08-20 Tue 19:55]
[2019-08-14 Wed]
The project unfortunately only runs on Ubuntu 18.04 and some other OSes, and not on my own machine...
I'm currently developing inside a VM.

The project currently consists of a minimal Makefile for a single enclave and app that runs in simulation mode.
Adding functions to the enclave requires their definition in the header file, and implementation, as for regular C code.
Additionally, it requires addition to the ~trusted~ block in [[file:Enclave/Enclave.edl::trusted%20{][Enclave/Enclave.edl]], with publicly accessible functions having the ~public~ keyword added to them.

In the app, the functions are then called with a slightly different signature.
The return value of enclave functions is always an sgx_status, and the ~global_eid~ is added to the parameters.
Return values of the function are set by the SGX wrapper through pointers, C-style.

**** DONE Extract Weights from neural net
     CLOSED: [2019-09-16 Mon 16:50]
     I can store the weights of a model in HDF5 format, and then load that using their C++ API.
     Then I could either load the weights from inside the enclave (which gives us no benefit at all), or hardcode them.

     I need the activation functions anyway, so hardcoding is probably the way to go.

     [2019-09-03 Tue 09:40]
     FUCK TENSORFLOW
     The C++ API requires building from source, and that requires bazel, and then everything together is a massive house of cards where the cards randomly self ignite, and then the example doesn't compile and nothing works.
     That seems like an unreasonable amount of super annoying work, which I don't want to do.

     So, instead I will try to create the NN in python, and create cython bindings for my C++ code.
     This probably means I will have to write a wrapper app for the enclave code, and maybe later make calls to the enclave directly.

     [2019-09-04 Wed 14:24]
     Does it have to be Tensorflow?
     PyTorch seems to do the same thing but nicer, because it gives us more granular access to the underlying matrices.

     [2019-09-16 Mon 14:39]
     I moved the code for executing the dense layers to their own function in the ~Net~ class.
     Now I can load the weights from the state_dict and then do the computation using custom built functions for the matrix multiplication.
     This should serve as a good starting point for moving that first to C and then to the SGX enclave.
     
     [2019-09-16 Mon 15:30]
     Finished moving the dense NN calculation to plain numpy.
     It's actually fairly simple, I just need to remember to transpose the weight matrix.
     Here's the source code for it:
#+BEGIN_SRC python
    def dense(self, x):
        state = self.state_dict()
        # breakpoint()
        w1 = state['fc1.weight'].detach().numpy().T
        b1 = state['fc1.bias'].detach().numpy()
        x = x.detach().numpy()
        tmp = np.matmul(x, w1) + b1
        x = np.maximum(tmp, 0)

        w2 = state['fc2.weight'].detach().numpy().T
        b2 = state['fc2.bias'].detach().numpy()
        tmp = np.matmul(x, w2) + b2
        x = np.maximum(tmp, 0)
        return torch.Tensor(x)
#+END_SRC
     
**** DONE Understand biases in Pytorch NNs
     CLOSED: [2019-09-16 Mon 16:50]
I need to understand how biases are used in pytorch, so I can correctly model that behaviour.

     [2019-09-16 Mon 16:50]
They are simply added after the input is multiplied with the weight.
Then the sum of the multiplication result and the biases is run through the nonlinearity.

     [2019-09-25 Wed 10:54]
The reason why I was so confused by biases in Pytorch is the fact that multiple inputs in a batch are ~hstacked~ together into a single matrix.
This means the computation the tensor does internally is more than simple matrix multiplication.
In order to do this correctly manually, one has to iterate over every column and compute the matrix multiplication for that column alone.
Then the dimensions of the bias vector also fit again.

**** DONE Write a script that generates C arrays from pytorch weight matrices
     CLOSED: [2019-09-16 Mon 16:51]
     Because the weights should be hardcoded into the enclave (this avoids decryption during loading for now), I need a script to extract weights from a ~.pt~ file and generate C array definitions from it.

     See [[./python/torch/gen_headers.py][this script]].

**** DONE Choose test network for first prototype
     CLOSED: [2019-09-05 Thu 10:54]
     Just use MNIST CNN, I have working code for torch

**** DONE Write naive matrix multiplication code in C [3/3]
     CLOSED: [2019-09-18 Wed 10:13]
Just simple stuff, including testing
- [X] matrix matrix multiplication
- [X] matrix matrix addition
- [X] nonlinearity

**** DONE Compare results of torch, numpy and my C code
     CLOSED: [2019-09-19 Thu 10:29]
There seem to be some differences in results that can't entirely be blamed on rounding errors.
The differences are currently in the range of e-7, which is more than numpy uses for its ~np.allclose~ function (e-8).
I will build a small network and then compare all three methods of computation, to see what is happening and where it starts to diverge.

[2019-09-19 Thu 10:26]
Finished evaluation, see [[file:test_correctness.py::fc%20=%20nn.Linear(IN_FEATURES,%20OUT_FEATURES,%20bias=False)][test_correctness.py]].
Even though the differences are around e-6, they seem to be normal distributed, and the max/min increases with increased matrix size (which strengthens my belief in the normal distribution).
I don't think it is a problem, and if it becomes one I already have a test script set up for evaluation.

**** DONE Move the fully connected layers to naive matrix multiplication in C
     CLOSED: [2019-09-19 Thu 10:29]
In the forward function of the network, instead of invoking ~nn.Linear~ I can call a C function.
This means that pytorch doesn't know how to backpropagate, but it doesn't need to anymore.

[2019-09-18 Wed 17:10]
The numpy and C variants do give slightly different results than the pure pytorch variant (in the e-7 range).
Jakob thinks this is more than just rounding errors, so I should check that out.
See [[*Compare results of torch, numpy and my C code][this TODO]]

**** DONE Combine C functions into one ~dense~ function
     CLOSED: [2019-09-19 Thu 12:06]
     This function can then be moved to the enclave, otherwise it leaks intermediate values

**** DONE Add compile_commands
     CLOSED: [2019-09-19 Thu 14:21]
Added a single recursive Makefile, which can be wrapped using bear

**** DONE Check the details of parameter passing into enclaves
     CLOSED: [2019-09-23 Mon 11:34]
Simply passing input pointers to the matrices doesn't work, so I will need to test this with some smaller examples
[2019-09-23 Mon 11:33]
Adding a [in, count=s] parameter for every array tells the SGX autogenerated wrapper how large the array should be, and it is then copied to enclave memory

**** DONE Do naive matrix multiplication inside the SGX enclave
     CLOSED: [2019-09-23 Mon 11:34]
Move the aforementioned code function to the enclave.
With a well enough defined interface this shouldn't be too much work

**** DONE Feed result back into pytorch, or calculate softmax
     CLOSED: [2019-09-19 Thu 14:25]
As we want this to be an oracle, I should execute the "softmax" (just taking the maximum)

[2019-09-19 Thu 14:21]
matutil_dense() already only returns the label

**** DONE Find a way to actually call the Enclave from PyTorch
     CLOSED: [2019-09-23 Mon 14:55]
Currently the enclave is not called from pytorch
The wrapper is called correctly, and then no error happens, but the enclave is not called.

[2019-09-23 Mon 14:22]
I had not initialized the enclave, and did not really output any errors in the enclave wrapper.
So yeah, this one's on me.

[2019-09-23 Mon 14:40]
Added some error handling to pymatutil.

*** Improvements for paper [3/9]
**** DONE Find out how model architectures are stored
     CLOSED: [2019-09-25 Wed 10:49]
We want to be able to specify a cut similar to current SOTA architecture visualization tools.
***** PyTorch
PyTorch uses the python classes for model definition, usually only storing the state_dict.
However, it is also possible to store the entire *module* where the model is defined, using Python's [[https://docs.python.org/3/library/pickle.html][pickle]] module.
Internally this does the same thing as keeping the file containing the python class and loading the state_dict, as it stores that file as well.
This means it is dependent on the containing folder structure, which might lead to some very weird errors.

PyTorch has no innate visualization tool, it instead is directed more at a programmer's view of things, as it allows for debugging the actual code of the ~forward~ function.
What one can do is export the model to [[ONNX][ONNX]] format, and then visualize it using something like [[https://github.com/PaddlePaddle/VisualDL][VisualDL]].

For our separation we could create some macro, wrapper, whatever that is then easily plugged into the definition of ~forward~.

***** Tensorflow
For Tensorflow, everything is in the session.
The graph that describes the model, the state of layers, optimizers etc.
The problem is that this way I'm not sure if my code would even work, as this would mean rebuilding everything so it works with Tensorflow.

Tensorflow visualizes its graphs using Tensorboard, and gets the info for that from the log directory.
A link to this is [[https://www.tensorflow.org/guide/graph_viz][here]].
I'm not sure how easy we can use Tensorboard for input, as Tensorflow in general is hard to edit.

***** Open Neural Network Exchange - <<ONNX>>
[[https://github.com/onnx/onnx][ONNX]] is meant to be a framework independent way of specifying models, complete with data types and operators.
There are ways to *export* to ONNX format from most common tools, but not many have a way of *importing*.
This makes it very difficult to use for our purposes, as my code would then still be framework dependent, just using an independent way to specify the model.

ONNX does have its own runtime, and I could try and move dense parts of that to the SGX enclave, but that would make comparison harder for the (I assume) not super broadly adopted ONNX runtime.

***** Keras
Keras tries to be a human readable extension sitting on top of Tensorflow, Theano etc.
The layers of Keras (and therefore any custom layers) can be written in pure python, and I think they can then also call C functions.

Storing models in Keras saves the architecture as well as the weights.
Anything that also contains weights is stored as an [[https://en.wikipedia.org/wiki/Hierarchical_Data_Format][HDF5]] file.
It's also possible to store the architecture alone as a json file.

Keras can do rudimentary visualization by using pydot, which uses [[https://github.com/pydot/pydot][pydot]] to interface with [[http://www.graphviz.org/][graphviz]].
The function for this is ~plot_model~ in ~keras.utils~.
Alternatively Keras offers a ~model.summary()~ function which prints a summary of all the layers.

For these reasons, *Keras* seems to be the best choice, offering the versatility and customizability we need along with nice tooling for training, storing and loading, as well as visualization.

**** DONE Design a splitting functionality for Keras [3/3]
     CLOSED: [2019-10-08 Tue 16:05]
RBO wants to have a good way of specifying split position and visualization of it before we try and run the code.
The best way to do this is writing a Keras layer container (similar to Sequential).
This way we have control over how the underlying layers function.

***** DONE Fix plotting for composed Keras models
      CLOSED: [2019-09-25 Wed 15:56]
Keras models can be composed from each other.
There is a concat function, but one model can actually just contain another model.
For both the ~model.summary()~ function and the ~plot~ function however, the contained model is not summarized recursively.
It is just listed as ~sequential_2~.
This will be the first step.

[2019-09-25 Wed 15:55]
~plot_model~ has a parameter for expanding submodules, called ~expand_nested~.
Setting this to true gives output like in [[file:python/mnist_cnn.png][this image]].

***** DONE Finish test code for MNIST in Keras
      CLOSED: [2019-09-25 Wed 16:25]
In order to evaluate I need fully working code for MNIST

***** DONE Write extension to Keras Sequential [5/5]
      CLOSED: [2019-10-08 Tue 16:05]
We want to have a container similar to Keras's ~Sequential~ model that can also generate C code for the contained layers.
It should be possible to iterate over all contained layers and store the ~output_shape~ along the way.
Then I also need to store the ~input_shape~ of the first layer and the weights and biases of all layers.
Getting the activation functions could be a bit trickier.
Here it is probably best to either call the C functions like the Keras names, or introduce a mapping.
I'm not sure right now if I can actually get the name of the activation function or not, which could increase difficulty.

[2019-09-25 Wed 18:32]
When executing a Keras Sequential (e.g. for ~predict()~) at the execution at one point starts calling backend functions.
This is something I can't really trace, and try and emulate this with my extension to ~Sequential~.
Due to this it's smarter to build a function that takes a sequential model and builds the C code from that.
This could then be wrapped in a custom Keras layer that calls the C code.
I would first however have to test whether calling C code from a Keras layer works at all.

[2019-09-26 Thu 10:52]
I took a debugging dive into the ~model.predict()~ function.
It's fairly easy to follow until Keras hands off execution to the Tensorflow backend.
The model is collapsed into a single function, which makes extensive use of the TF session model (ugh...).
This function is then wrapped into a Keras ~Function~ object, which is also a TF ~GraphExecutionFunction~ object when using the TF backend.

****** DONE Test control flow for custom Keras layers
       CLOSED: [2019-09-26 Thu 12:02]
[2019-09-26 Thu 11:54]
Keras only sits on top of whatever backend it uses.
This means that any custom Keras layer really only generates TF tensors.
The contents of their [[https://keras.io/layers/writing-your-own-keras-layers/][~call()~]] function are really only called once, where they are converted to TF Graphexecution functions.
Also, the ~x~ input is a backend tensor, so there is not really any way to call my C function.
For that I would need the actual values, which they don't have at that point, because they are only placeholders...

The real problem is the inherent lazyness of TF 1.14.
*HOWEVER*, there is a way to add eager functions to it, by wrapping them with [[https://www.tensorflow.org/api_docs/python/tf/py_function][~tf.py_func~]].
This is a tensor wrapping a regular C function.
With this I can wrap my ~pymatutil~ function in a regular python function, taking numpy arrays as input.

Keras doesn't really have an equivalent.
I'm currently not sure whether it's smarter to switch to TF in general or just use the TF function in Keras.
I will check how easy it is to get weights out of TF NNs.

With TF 2.0 (which is currently available as a beta) eager execution is the default, so this would make things a lot easier.

[2019-09-26 Thu 16:00]
So I switched to TF 2.0.0 rc2, because the eagerness seems to be exactly what I want.
Unfortunately, during training it still switches to lazy execution, I guess for performance.
However, once everything is done the environment is in eager mode again.
I guess they never meant eager execution to work during full blown execution of the network.
*HOWEVER*, this still helps me to write my code, as I can use the Keras bundled with TF, and use [[https://www.tensorflow.org/api_docs/python/tf/py_function][~tf.py_func~]] to call my code.
Will test tomorrow.

****** DONE Use ~tf.py_func~ to call C code for testing
       CLOSED: [2019-10-01 Tue 09:58]
It should probably be a simple operation like multiplying with 2.

****** DONE Test calling C Code from a custom Keras layer
       CLOSED: [2019-09-26 Thu 11:57]
Does not make sense, see [[*Test control flow for custom Keras layers][this point]], lazyness is super annoying.

[2019-09-27 Fri 09:56]
Actually, with TF as backend this still works (see [[file:python/tf_poc.py::return%20tf.py_function(func=external_func,%20inp=%5Bx%5D,%20Tout=tf.float32)][this code]]).
The input to ~Layer.call()~ is a TF tensor in that case, and I can use that as an input to a ~tf.py_function~.
This ~tf.py_function~ can then call my interop code.

****** DONE Extract weights, biases, etc.
       CLOSED: [2019-10-01 Tue 09:58]
Currently just for Keras, will be done by extending ~Sequential~

****** DONE Generate ~dense()~
       CLOSED: [2019-10-01 Tue 09:58]
Generate the calling code, currently without any template engine.

[2019-10-01 Tue 09:58]
The ~Enclave~ extension of ~Sequential~ has a ~generate_dense()~ function that will build the dense function out of matutil functions.
This is done via format strings, which is not the best solution, but it works for now.

**** DONE Build custom Keras layer for C code execution
     CLOSED: [2019-10-01 Tue 10:00]
After the C code is generated, the custom Sequential model should either call that instead of the underlying layers, or replace itself with a new layer.
That new layer would then either call the Enclave or native C code.

[2019-10-01 Tue 09:59]
This is done by creating a custom Layer that builds a ~tf.py_function~, which calls the python function calling my C interop code.

**** TODO Test on actual hardware [16/21]                               :sgx:
We want to test this on some actual hardware.
For a representative evaluation we should use one consumer grade, and one server grade CPU.
According to [[https://software.intel.com/en-us/forums/intel-software-guard-extensions-intel-sgx/topic/606636][this forum answer]] Intel SGX was introduced with Skylake generation CPUs, so any 6xxx processor will be fine.

***** DONE Write an e-mail to Manuel
      CLOSED: [2019-09-25 Wed 13:31]
***** DONE Set up the machine
      CLOSED: [2019-10-08 Tue 15:25]
I have a [[file:setup_sgx_machine.sh][setup script]] for Ubuntu 19.04, which I tested on VMs.

[2019-10-04 Fri 12:00]
The hardware is set up, it only needs a setup of Ubuntu, and network access.
***** DONE Plan evaluation
      CLOSED: [2019-10-11 Fri 11:32]
****** DONE Coordinate with RBO how exactly we evaluate                 :sgx:
       CLOSED: [2019-10-08 Tue 17:29]
I work out the exact benchmarking details with him.
He said he wanted to test different cutting points, so I should also add Convolutional layers to the enclave.

Convolutional layers in the front.
Ideally more than two dense layers in the end.
We can look at ImageNet structures, which should have more dense layers.

***** CANCELED Get VGG Face Dataset up and running
      CLOSED: [2019-10-29 Tue 16:31] SCHEDULED: <2019-11-04 Mon>
This is large enough to merit its own project, and it's currently underway

[2019-10-29 Tue 11:55]
We are currently at 229 ... images out of over 2 6.. ..., so slightly under 10%.
We're getting there.

[2019-10-29 Tue 16:31]
We're still doing VGG face for its own sake, but it's not good enough (with all the missing images) for our current use-case.

***** DONE Prepare evaluation using TF built-in datasets
       CLOSED: [2019-10-15 Tue 11:54] SCHEDULED: <2019-10-15 Tue 12:00>
TF has a built in, pretrained version of VGG16.
It also has multiple built-in datasets which can be used for retraining the dense layers.
This can be used as a preparation for the evaluation.

[2019-10-15 Tue 11:41]
The tensorflow_datasets library is a bit weird to use, so for now I'm sticking to a dataset that's downloaded from the tensorflow servers.

[2019-10-15 Tue 11:48]
Added evaluation scripts for both versions of vgg_flowers.
Now only the timing needs to be set up.

***** DONE Think about good ways to measure inference time
      SCHEDULED: <2019-10-23 Wed>
We have one-time setups that take place, which need to be measured.
There is also additional communication between the GPU and CPU that needs to take place.

We should measure:
- single inference time, which should include enclave setup
- time for a batch predict, so we have a better understanding of how the enclave performs over larger datasets
- check if the entire model fits in the 128MB that we have

[2019-10-18 Fri 23:10]
This new approach to using TF dataset genartors requires some switchup to the method.
I will change the evaluation script so it provides a method you can call, providing it the model and the generators.
It will give me a fixed timing framework for the benchmarking project.

[2019-10-21 Mon 15:43]
Dataset generators are done.

***** DONE Add timing code to evaluation script
      CLOSED: [2019-10-29 Tue 12:06] SCHEDULED: <2019-10-28 Mon>
Probably just a simple timestamp before and after.
It's probably best to use system time, as using only active CPU time would miss GPU time.

***** DONE Convert trained VGG flowers model to enclave
       CLOSED: [2019-10-16 Wed 15:30] SCHEDULED: <2019-10-16 Wed>
[2019-10-15 Tue 15:35]
The original model with two layers of 4096 neurons is too large to reasonably convert using my current toolchain.
They are also too large to fit in the enclave without paging.
I reduced the number of neurons in these layers to 2048 and requeued the training, it's currently enqueued.

[2019-10-15 Tue 17:14]
I'm currently running into the problem that the weight matrices are just too large.
I split the first weight matrix into its own file, and tried to compile only that, but the compilation took up all 16GB of RAM available on my machine.
The next step would be to look into building ELF files by hand, or simply switching to a more powerful machine.

[2019-10-16 Wed 09:31]
I'm currently looking into what the problem with the state is.
My intuition says that it's even before the actual compilation.
To verify this, I'm using clang with the ~-emit-ast~ option.

If this goes through, then it could be the constant table that's being built during compilation.
I might be able to turn this off during compilation, but I'm not sure.

[2019-10-16 Wed 15:28]
I changed the output so it generates binary files containing the weight matrices.
The code now compiles without issues.
However, it's still too large for the enclave

***** DONE Train VGG flowers model
       CLOSED: [2019-10-21 Mon 09:23]
This is a necessary first step before training the VGG face model

[2019-10-21 Mon 09:23]
I have a trained model using a GlobalAveragePooling2D layer instead of flattening.
This greatly reduces the number of trainable weights, which is good because I don't have enough data for the larger number anyway.
For the much larger VGG face dataset this might be different.

***** DONE Install CUDA and tf-gpu on SGX machine
      CLOSED: [2019-10-23 Wed 15:25] SCHEDULED: <2019-10-22 Tue>
For a valid benchmark we need to compare it to the GPU version of tensorflow.

***** CANCELED Train a smaller VGG face model with 10 classes
      CLOSED: [2019-10-31 Thu 15:23]
This should make it reasonably close to the VGG flowers model, but still relevant enough for security so we can use it.

[2019-10-31 Thu 15:23]
The dataset has changed, see [[LFW][here]]
***** DONE Resolve the problem of too large networks
      CLOSED: [2019-10-31 Thu 15:25]
The network as it is doesn't fit in the enclave.
I should first look at the ~GlobalAveragingLayer~ (or similar) in TF.
This might give us the much needed size reduction.

[2019-10-16 Wed 15:50]
I just checked the included VGG16 model, whose structure looks like the following:
#+BEGIN_SRC python :results output
  from tensorflow.keras.applications import VGG16
  VGG16().summary()
#+END_SRC

#+RESULTS:
#+begin_example
Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 224, 224, 3)]     0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544 
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
predictions (Dense)          (None, 1000)              4097000   
=================================================================
Total params: 138,357,544
Trainable params: 138,357,544
Non-trainable params: 0
_________________________________________________________________
#+end_example

As you can see, there are 25088 inputs to the dense part of the network.
This does require some additional reduction before I can use it.

[2019-10-21 Mon 09:25]
Rainer suggested adding an additional dense layer before the enclave to reduce dimensionality.
This might work, but it also reduces the number of neurons hidden from the network host.
We can probably argue that the shape of the enclave network is irrelevant, only the number of hidden neurons is important.
Then we can treat the shape of the additional dense layer as a "security parameter" (which it technically isn't but that's ok)

******* DONE try an ~GlobalAveragePooling2D~ in vgg_flowers code
	CLOSED: [2019-10-21 Mon 09:28]
As a hopefully better measure I will try to add a global average pooling layer before the flatten.
This will greatly reduce dimensionality, and seems to be a SOTA technique.
Hopefully tests from the flower dataset generalize to the face dataset.

[2019-10-21 Mon 09:28]
This does work well on the flowers dataset.
More tests with the face dataset coming.

***** DONE Get MS face dataset
      CLOSED: [2019-10-23 Wed 17:09]
There is an academic torrent for the [[http://academictorrents.com/details/9e67eb7cc23c9417f39778a8e06cca5e26196a97][MS-Celeb-1M]] dataset.
While this isn't VGG face it is a face recognition database by Microsoft, so benchmarking on it seems fine.
We can use this in case the VGG face download doesn't work out fast enough.

***** CANCELED Extract images from MS face dataset
      CLOSED: [2019-11-04 Mon 09:18]
The images in the MS dataset are contained as base64 blobs (I think).
Some assembly required...

******* DONE Get more disk space for the sgx machine
	CLOSED: [2019-10-21 Mon 12:08] SCHEDULED: <2019-10-21 Mon>
Downloading the celebrity dataset requires more disk space than the test machine currently has.
I will talk to Manuel about an additional HDD.

[2019-10-21 Mon 12:08]
Manuel gave me an external 3TB hard drive.
An internal one is already ordered, it will be here this week.

******* DONE Check the disk space on the cluster
	CLOSED: [2019-10-21 Mon 09:35]
I'm not sure if I could move the entire dataset to the cluster, which would mean I can't train there.

[2019-10-21 Mon 09:34]
The main disk has 457G of space, so moving the large dataset there is probably not prudent.
There is an HDD raid, but I don't have permissions to access that at the moment.
I should ask the admins about it.

******* CANCELED Ask about getting access to the HDD Raid on the cluster
	CLOSED: [2019-10-21 Mon 09:58]
This would be the best way IMO to use large datasets on the cluster.

[2019-10-21 Mon 09:58]
The resized dataset is not very large anymore, so this isn't really necessary at the moment

***** DONE Talk to Rainer about the reduced class subset of VGG face    :sgx:
       CLOSED: [2019-10-29 Tue 16:30] SCHEDULED: <2019-10-29 Tue>
      [2019-10-29 Tue 08:56]

    We currently have 222 classes (221 of which should downloaded as much as possible)
    This seems like a good starting point for a classifier

***** DONE [#A] Get ANY dataset working
      CLOSED: [2019-10-30 Wed 17:08] SCHEDULED: <2019-10-30 Wed>
[2019-10-29 Tue 17:32]
[[https://megapixels.cc/datasets/msceleb/][This]] is a pretty damning report about the MS Face dataset.
We could still use it, but we should be aware of the things happening around it.
It also links to [[https://ibug.doc.ic.ac.uk/resources/lightweight-face-recognition-challenge-workshop/][this workshop]], where ther is a download link for a reduced and cleaned up dataset extracted from it.

[2019-10-29 Tue 17:52]
The labeled faces in the wild dataset is contained in ~tensorflow_datasets~ and seems to work.
There is a git repo [[https://github.com/davidsandberg/facenet][here]] which implements the model from [[https://arxiv.org/pdf/1503.03832.pdf][this paper]].
However, the author of the repo doesn't seem to be affiliated with the authors of the paper.
LFW in general at least seems like a promising direction.

<<LFW>>
[2019-10-30 Wed 17:08]
I started training a classifier on LFW

***** TODO Get NN above 80% on a 100 class image recognition task
The images should be 224x224 (or larger), and the convolutional part should be based on VGG (or ResNet).
I'm currently working the 15 to 115 most common classes in the LFW dataset, the model is still running.
My best result to date is 65%.

[2019-11-05 Tue 16:15]
I just fixed a bug in the dataset generation that breaks everything.
I'll let the currently running instance finish, but will schedule a new VGG and ResNet run.
Hopefully there will be better results tomorrow.

****** VGG-LFW
I pick the 100 most common classes, dropping the most common class.
The most common class is dropped to somewhat reduce variation in the prior probabilities (the most common class has over 500 samples, the next has 100).

******* "Baseline"
The current baseline has 4096 hidden neurons, and is trained for 100 epochs.
It reached 61% once, but I since then changed some dataset stuff, so I'm not sure if that is correct anymore.

******** WATING Retrain baseline model
	 SCHEDULED: <2019-11-06 Wed 15:00>
I changed some dataset generation code, I should rerun the baseline to still have an accurate and current baseline.

[2019-11-06 Wed 12:56]
Just started, it's job 11908.

[2019-11-06 Wed 18:32]
The new baseline gets 55% accuracy, log output:
#+include: "cluster_outputs/baseline.log" src text

******* Larger variants
I tried increasing the number of hidden neurons and increasing the number of training epochs.
Both variants slightly increased the performance (larger 62%, longer 62%, all combined 65%).
However, that doesn't seem like the right thing to do, because my results already look like they're overfitting too much.

******** TODO Train flattened model for 50 epochs
	 SCHEDULED: <2019-11-07 Thu>
I will try the model which takes the flattened features as output instead of the GlobalAveragePooling2D.
This time I will train it for 50 epochs, and on the GPU, it might be quite fast.

******* Smaller variants
I will also try running smaller variants, one with only 50 training epochs, and one with only 512 hidden neurons.
We'll see how that goes.

******** DONE Train 50 epoch model
	 CLOSED: [2019-11-06 Wed 15:38] SCHEDULED: <2019-11-06 Wed 15:00>
It's job 11909.
This is the log output, it gets 58% accuracy:
#+include: "cluster_outputs/50_epochs.log" src text

It reaches 58% accuracy for the test set, 94% for the training set.

******** WATING Train 512 neuron model
	 SCHEDULED: <2019-11-07 Thu 15:00>
Just started it, job 11917 on the cluster.
This one will only train for 50 epochs.
Log output is here, it gets 54% accuracy:
#+include: "cluster_outputs/512_neurons_50_epochs.log" src text

I will train the same model on the SGX machine overnight, this time for 100 epochs.
This will let me know if training works on that machine, also it gives me some more results.


***** TODO Get Ember dataset working
We now have a working image dataset for benchmarking.
Ideally we get a working second media type that also uses CNNs working, for better results.

[2019-11-05 Tue 16:01]
I started on working with the Ember dataset.
The features are extracted from the raw data, I just don't know that they mean yet...

***** TODO Train classifier on Benchmark Data
Currently one of the benchmark sets is the 100 most common classes in labelled faces in the wild (LFW).
We also need a second dataset, maybe something on malware
      
****** TODO Test different vgg_lfw variants                             :sgx:
      [2019-11-04 Mon 08:35]

The baseline model reaches 61% validation accuracy after 100 epochs.
See if this can be improved via:
- increasing training epochs: 62%
- dense on flattened: 63%
- globalmean with more dense neurons: 62%
- more dense neurons on flattened: 
***** TODO Measure performances for multiple network types
enclave vs. GPU for multiple cutoff points
Test with enclave and with native C code

MNIST, one more type of data

****** DONE get VGG16 model
	CLOSED: [2019-10-15 Tue 08:53]
There is one included within ~keras.applications~.
I think we should use it, with standard TF datasets.

[2019-10-15 Tue 08:53]
We are probably using that network, with the pre-trained weights.
However the end goal will be to evaluate it on the VGG face datset, when that is available to us.

***** TODO Plan privacy evaluation
more on this later

**** TODO Add capability for convolutional layers :sgx:
     
**** TODO Test the splitting with several popular architectures :sgx:
We should test the splitting with several common architectures.
RBO also said something about splitting according to some metrics (e.g. GPU memory/utilization), which I'm not too sure about how he meant it.
We could try and split before the first dense layer, I should try and see if there are other layer types used for actual classification.

**** TODO Automate memory layout inside SGX :sgx:
We might have to do some memory magic because otherwise we might run out of memory inside the SGX.
The first prototype can do this explicitly for the chosen network, but for publishing we should do this automatically.

Alternative we could also generate a fixed function from sparse matrix multiplication.
This would mean that we go through the output cell by cell, calculating all immediate steps in a row.
Using this would throw away any shared results, and be much slower.
However, this could help us avoid memory issues.

[2019-09-25 Wed 10:56]
This has to wait until we are testing on actual hardware.
For the simulation mode we are currently running, memory doesn't seem to be an issue.
I'm currently specifying intermediate matrices after every ~fc~ layer of the network (see [[file:lib/sgx/Enclave/Enclave.cpp][the enclave code]]), without any issues.

This either means we are not as memory constrained as we thought, or the simulation simply has a larger amount of memory than the actual enclave would have.

[2019-10-08 Tue 16:09]
The enclave has 100MB of memory.
This might require some trickery for full models.
I could try and use only two temporary buffers, to reduce memory requirements.

**** TODO Anwendungsfälle auflisten                                     :sgx:
    [2019-10-04 Fri 11:36]
***** Model stealing
****** DONE Find some papers on model stealing                          :sgx:
       CLOSED: [2019-10-23 Wed 17:20]

***** Offline ML-as-a-service <<Offline MLAAS>>
Making content of ~dense()~ and network weights independent of launch code would allow MLAAS providers to sign an execution environment once, and then give signed and encrypted weights and architectures to their customers.
This would require decrypting the models and dynamically generating the ~dense()~ function.

By doing so we can move MLAAS from the cloud to offline, allowing for easier use, at least on desktop devices.
While this doesn't necessarily work for mobile devices, it would allow users of MLAAS services to run the models themselves.

The resulting classification could even be signed together with the input, thus verifying that the classification came from a certain model.
*This would add accountability to every single classification.*

Maybe we can spin an additional paper from this.

***** Oblivious distributed machine learning
Along the lines of [[Offline MLAAS][this]] and distributed SGD, we can do the same thing the other way round.
Companies could send signed and encrypted inputs, together with model updates to people offering their compute power.
The people then train their local model inside the enclave, so they can't steal the model.
The code in the enclave then signs and encrypts the weight updates and sends them back to the company.

The local devices would all use the same signature if they are provided with the same enclave code, so they wouldn't be identifiable from their updates alone.
This can be cited from [[file:related_work/shokri15privacy.pdf][Shokri ML privacy]], the learning in general would be similar to [[file:related_work/dean2012large.pdf][Downpour]].

This would make an *awesome* paper.

***** Privacy -> Set Membership
I should do the math where the total attack accuracy for only returning the label comes from in the set membership paper.
Other than that we still have the advantage of making an offline model as robust as an online oracle.

****** TODO Leakage confinement
Find out if the leakage information is constrained to the dense layers.
Figure -> dense weights before and after retraining

[2019-10-31 Thu 15:29]
I mean it's contained in the dense layers if you keep it there.
What would be interesting is how much the convolutional layers change if you don't fix them.
Here a delta per individual weight would be interesting.

A comparison of accuracies on the same dataset with and without freezing the convolutional layers would also be beneficial.
This relates to [[Layer freezing in transfer learning][transfer learning]].

****** TODO Run on Cecilias face classifier
Retraining, then try and see where the differences are during retraining.
Evaluate set membership.

***** TODO consider possible new attacks on partial black boxes
An attacker might use the early convolutional layers to reduce the search space for adversarial examples.
This needs to be in the final paper.

***** Lastverteilung zw. Edge, Cloud
****** TODO Find some papers on load balancing for distributed heterogenous architectures :sgx:
Or talk to DPS groups

**** Read related work [5/6]                                  :sgx:
    [2019-10-03 Thu 10:15]

***** DONE Data privacy:
      CLOSED: [2019-10-08 Tue 16:05]
One is [[file:~/Projects/nn-sgx/related_work/shokri15privacy.pdf][on privacy preserving deep learning (leakage)]], which provides a way to perform SGD distributed over multiple nodes.
By not sending all state updates to the state server the participating parties retain privacy of their dataset.
This is cool in the sense that it solves a problem that we could also try and solve with our method (which would give us [[file:related_work/ohrimenko16enclave.pdf][the ohmirenko paper]] exactly).
Other than that there's not too much in common.

The other Shokri paper is [[file:~/Projects/nn-sgx/related_work/shokri17membership.pdf][on membership attacks]].
However, in my opininion this doesn't have too much to do with our case.
The case for k=1 label, however is weird.
Just guessing gives the correct accuracy, but precision and recall are a bit weird.
I should talk to Rainer about this

****** DONE Talk to Rainer about the k=1 label case for set membership attacks
       CLOSED: [2019-10-29 Tue 08:55] SCHEDULED: <2019-10-28 Mon>

***** DONE Enclave Security                                             :sgx:
      CLOSED: [2019-10-21 Mon 10:07]
[[file+sys:related_work/kaptchuk2019state.pdf][This paper]] by Kaptchuk et al. uses ledgers to add state to the enclave, which guarantees that every state is only presented to it once.

We could use this mechanism to implement rate limiting on our models.
This would mean they aren't truly offline anymore, but I guess that's the security/usability tradeoff we have to face.

***** DONE NNs in TEEs
      CLOSED: [2019-10-15 Tue 09:51] SCHEDULED: <2019-10-15 Tue 12:00>
Dan Boneh released [[file:related_work/tramer19slalom.pdf][SLALOM]] with Florian Tramer, talking about something very similar to what we're doing.

[2019-10-15 Tue 10:50]
They do talk about something similar to what we're doing, but they're doing it very differently.
Instead of running parts of the NN in the enclave, they run it on the GPU and verify its outputs in the enclave.
Also they apply some encryption mechanism to the input data to keep it private from the model owner.

The notion of verifying outputs that come from the GPU is very cool, and we should look into something similar for our project.
Encrypting the input might also be a posibility for future work.

[2019-10-26 Sat 19:10]
In 2016 Ohrimenko et al. released a paper on [[file:related_work/ohrimenko16enclave.pdf][training different ML flavors in the enclave]].
Their focus is defending against power analysis attacks to keep the training data private from some third party datacenter.
Not entirely related to our method.

Also what they do requires the entire training process to happen in the enclave, which is a lot slower than training on GPUs, which we could do.

***** DONE [#A] Model stealing
      CLOSED: [2019-10-26 Sat 18:37] SCHEDULED: <2019-10-24 Thu>
A good paper on model stealing seems to be [[file:related_work/tramer16stealing.pdf][this one]] by Tramer et al., which Daniel recommended to me.
The query budget they set for their attack is ~100*k~, where ~k~ is the number of parameters.
The dense part of our model has 18903.045 parameters, so that wouldn't be feasible.

***** TODO Find something on accountability for NNs
[2019-10-26 Sat 17:58]
What I found is more about accountability for [[http://merrill.umd.edu/wp-content/uploads/2016/01/p56-diakopoulos.pdf][automatically generated textual reports]].
A [[https://officialblogofunio.com/2019/04/29/a-short-introduction-to-accountability-in-machine-learning-algorithms-under-the-gdpr/%0A][blog post]] (that is heavily focused on law) talks about accountability of data processing under GDPR.
One could think of a use case where a model is trained, tested for differential privacy and after passing that testing it is approved and signed.
The legislator could then co-sign the code running in the enclave, like a certificate.
Whoever runs the model then can provide proof that the model they are running is differentially private, if that model signs it's outputs.

***** DONE Layer freezing in transfer learning
      CLOSED: [2019-11-04 Mon 18:01] SCHEDULED: <2019-11-04 Mon 18:00>
Cecilia pointed me towards transfer learning.
There should be some papers on this especially in the context of layer freezing.
I should talk to her next week

[2019-11-04 Mon 09:24]
Cecialia sent me an email with resources, I should check them out.

****** TODO Talk to Cecilia about Layer Freezing
After me reading some literature we might be able to have a good discussion.

*** Future Work [0/2]
**** TODO Integrate with the framework API :sgx:
Rainer said it would be nice to integrate the SGX with the tensorflow API (or pytorch, whatever)

**** TODO Examine required query counts for transfer and full model stealing attacks
Transfer attacks require much less precise replications of the target model.
It would be interesting to see how the query numbers differ for actual model stealing and transfer attacks.

** Unforeseen events
*** DONE Figure out how many bits the enclave uses for floats
    CLOSED: [2019-10-04 Fri 10:11]
This could cause some weird results and incompatibilities.
It's also not perfectly clear if the enclave even supports floats.

[2019-10-04 Fri 10:10]
The enclave supports floats.
In all my tests there have been rounding errors (larger than the ~numpy~ default of 10^-8), but the results are identical.
I think this is good enough.

*** DONE Find out what is needed for the foreshadow mitigation          :sgx:
    CLOSED: [2019-10-04 Fri 10:10]
   [2019-10-04 Fri 10:02]
 I should probably read [[https://foreshadowattack.eu/foreshadow.pdf][the paper]].
 According to [[https://foreshadowattack.eu/][the attack's website]] (what a world we live in), under the headline "Are there mitigations against Foreshadow?", mitigations require software and microcode updates.
 I would assume the software updates are for changing keys (which could have been leaked by vulnerable systems) and resealing enclave code.

 The microcode is for patching the vulnerability.
 Intel released a [[https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00161.html][security advisory]] on foreshadow, as well as a summary of microcode updates.
 This means that having currenty microcode updates on our benchmark machines would make them resistant against foreshadow.
 At least as resistant as we can currently get.

*** TODO Test if our processors are vulnerable to foreshadow            :sgx:
    [2019-10-04 Fri 11:46]

If they are, don't update, we want to measure performance impact of that 
Updating is probably ~irreversible~, so we should find out if they are vulnerable before installing any microcode packages.

