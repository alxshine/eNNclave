* NN SGX
Running the dense part of CNNs inside the trusted enclave to reduce leakage and protect against model stealing.
We hope to make this as robust against model stealing as online oracles.

** People
RBO, CPA, ASC

** Project Outline
*** First prototype [14/14]
**** DONE Create working environment for Testing and Development
     CLOSED: [2019-08-20 Tue 19:55]
[2019-08-14 Wed]
The project unfortunately only runs on Ubuntu 18.04 and some other OSes, and not on my own machine...
I'm currently developing inside a VM.

The project currently consists of a minimal Makefile for a single enclave and app that runs in simulation mode.
Adding functions to the enclave requires their definition in the header file, and implementation, as for regular C code.
Additionally, it requires addition to the ~trusted~ block in [[file:Enclave/Enclave.edl::trusted%20{][Enclave/Enclave.edl]], with publicly accessible functions having the ~public~ keyword added to them.

In the app, the functions are then called with a slightly different signature.
The return value of enclave functions is always an sgx_status, and the ~global_eid~ is added to the parameters.
Return values of the function are set by the SGX wrapper through pointers, C-style.

**** DONE Extract Weights from neural net
     CLOSED: [2019-09-16 Mon 16:50]
     I can store the weights of a model in HDF5 format, and then load that using their C++ API.
     Then I could either load the weights from inside the enclave (which gives us no benefit at all), or hardcode them.

     I need the activation functions anyway, so hardcoding is probably the way to go.

     [2019-09-03 Tue 09:40]
     FUCK TENSORFLOW
     The C++ API requires building from source, and that requires bazel, and then everything together is a massive house of cards where the cards randomly self ignite, and then the example doesn't compile and nothing works.
     That seems like an unreasonable amount of super annoying work, which I don't want to do.

     So, instead I will try to create the NN in python, and create cython bindings for my C++ code.
     This probably means I will have to write a wrapper app for the enclave code, and maybe later make calls to the enclave directly.

     [2019-09-04 Wed 14:24]
     Does it have to be Tensorflow?
     PyTorch seems to do the same thing but nicer, because it gives us more granular access to the underlying matrices.

     [2019-09-16 Mon 14:39]
     I moved the code for executing the dense layers to their own function in the ~Net~ class.
     Now I can load the weights from the state_dict and then do the computation using custom built functions for the matrix multiplication.
     This should serve as a good starting point for moving that first to C and then to the SGX enclave.
     
     [2019-09-16 Mon 15:30]
     Finished moving the dense NN calculation to plain numpy.
     It's actually fairly simple, I just need to remember to transpose the weight matrix.
     Here's the source code for it:
#+BEGIN_SRC python
    def dense(self, x):
        state = self.state_dict()
        # breakpoint()
        w1 = state['fc1.weight'].detach().numpy().T
        b1 = state['fc1.bias'].detach().numpy()
        x = x.detach().numpy()
        tmp = np.matmul(x, w1) + b1
        x = np.maximum(tmp, 0)

        w2 = state['fc2.weight'].detach().numpy().T
        b2 = state['fc2.bias'].detach().numpy()
        tmp = np.matmul(x, w2) + b2
        x = np.maximum(tmp, 0)
        return torch.Tensor(x)
#+END_SRC
     
**** DONE Understand biases in Pytorch NNs
     CLOSED: [2019-09-16 Mon 16:50]
I need to understand how biases are used in pytorch, so I can correctly model that behaviour.

     [2019-09-16 Mon 16:50]
They are simply added after the input is multiplied with the weight.
Then the sum of the multiplication result and the biases is run through the nonlinearity.

     [2019-09-25 Wed 10:54]
The reason why I was so confused by biases in Pytorch is the fact that multiple inputs in a batch are ~hstacked~ together into a single matrix.
This means the computation the tensor does internally is more than simple matrix multiplication.
In order to do this correctly manually, one has to iterate over every column and compute the matrix multiplication for that column alone.
Then the dimensions of the bias vector also fit again.

**** DONE Write a script that generates C arrays from pytorch weight matrices
     CLOSED: [2019-09-16 Mon 16:51]
     Because the weights should be hardcoded into the enclave (this avoids decryption during loading for now), I need a script to extract weights from a ~.pt~ file and generate C array definitions from it.

     See [[./python/torch/gen_headers.py][this script]].

**** DONE Choose test network for first prototype
     CLOSED: [2019-09-05 Thu 10:54]
     Just use MNIST CNN, I have working code for torch

**** DONE Write naive matrix multiplication code in C [3/3]
     CLOSED: [2019-09-18 Wed 10:13]
Just simple stuff, including testing
- [X] matrix matrix multiplication
- [X] matrix matrix addition
- [X] nonlinearity

**** DONE Compare results of torch, numpy and my C code
     CLOSED: [2019-09-19 Thu 10:29]
There seem to be some differences in results that can't entirely be blamed on rounding errors.
The differences are currently in the range of e-7, which is more than numpy uses for its ~np.allclose~ function (e-8).
I will build a small network and then compare all three methods of computation, to see what is happening and where it starts to diverge.

[2019-09-19 Thu 10:26]
Finished evaluation, see [[file:test_correctness.py::fc%20=%20nn.Linear(IN_FEATURES,%20OUT_FEATURES,%20bias=False)][test_correctness.py]].
Even though the differences are around e-6, they seem to be normal distributed, and the max/min increases with increased matrix size (which strengthens my belief in the normal distribution).
I don't think it is a problem, and if it becomes one I already have a test script set up for evaluation.

**** DONE Move the fully connected layers to naive matrix multiplication in C
     CLOSED: [2019-09-19 Thu 10:29]
In the forward function of the network, instead of invoking ~nn.Linear~ I can call a C function.
This means that pytorch doesn't know how to backpropagate, but it doesn't need to anymore.

[2019-09-18 Wed 17:10]
The numpy and C variants do give slightly different results than the pure pytorch variant (in the e-7 range).
Jakob thinks this is more than just rounding errors, so I should check that out.
See [[*Compare results of torch, numpy and my C code][this TODO]]

**** DONE Combine C functions into one ~dense~ function
     CLOSED: [2019-09-19 Thu 12:06]
     This function can then be moved to the enclave, otherwise it leaks intermediate values

**** DONE Add compile_commands
     CLOSED: [2019-09-19 Thu 14:21]
Added a single recursive Makefile, which can be wrapped using bear

**** DONE Check the details of parameter passing into enclaves
     CLOSED: [2019-09-23 Mon 11:34]
Simply passing input pointers to the matrices doesn't work, so I will need to test this with some smaller examples
[2019-09-23 Mon 11:33]
Adding a [in, count=s] parameter for every array tells the SGX autogenerated wrapper how large the array should be, and it is then copied to enclave memory

**** DONE Do naive matrix multiplication inside the SGX enclave
     CLOSED: [2019-09-23 Mon 11:34]
Move the aforementioned code function to the enclave.
With a well enough defined interface this shouldn't be too much work

**** DONE Feed result back into pytorch, or calculate softmax
     CLOSED: [2019-09-19 Thu 14:25]
As we want this to be an oracle, I should execute the "softmax" (just taking the maximum)

[2019-09-19 Thu 14:21]
matutil_dense() already only returns the label

**** DONE Find a way to actually call the Enclave from PyTorch
     CLOSED: [2019-09-23 Mon 14:55]
Currently the enclave is not called from pytorch
The wrapper is called correctly, and then no error happens, but the enclave is not called.

[2019-09-23 Mon 14:22]
I had not initialized the enclave, and did not really output any errors in the enclave wrapper.
So yeah, this one's on me.

[2019-09-23 Mon 14:40]
Added some error handling to pymatutil.

*** Improvements for paper [3/9]
**** DONE Find out how model architectures are stored
     CLOSED: [2019-09-25 Wed 10:49]
We want to be able to specify a cut similar to current SOTA architecture visualization tools.
***** PyTorch
PyTorch uses the python classes for model definition, usually only storing the state_dict.
However, it is also possible to store the entire *module* where the model is defined, using Python's [[https://docs.python.org/3/library/pickle.html][pickle]] module.
Internally this does the same thing as keeping the file containing the python class and loading the state_dict, as it stores that file as well.
This means it is dependent on the containing folder structure, which might lead to some very weird errors.

PyTorch has no innate visualization tool, it instead is directed more at a programmer's view of things, as it allows for debugging the actual code of the ~forward~ function.
What one can do is export the model to [[ONNX][ONNX]] format, and then visualize it using something like [[https://github.com/PaddlePaddle/VisualDL][VisualDL]].

For our separation we could create some macro, wrapper, whatever that is then easily plugged into the definition of ~forward~.

***** Tensorflow
For Tensorflow, everything is in the session.
The graph that describes the model, the state of layers, optimizers etc.
The problem is that this way I'm not sure if my code would even work, as this would mean rebuilding everything so it works with Tensorflow.

Tensorflow visualizes its graphs using Tensorboard, and gets the info for that from the log directory.
A link to this is [[https://www.tensorflow.org/guide/graph_viz][here]].
I'm not sure how easy we can use Tensorboard for input, as Tensorflow in general is hard to edit.

***** Open Neural Network Exchange - <<ONNX>>
[[https://github.com/onnx/onnx][ONNX]] is meant to be a framework independent way of specifying models, complete with data types and operators.
There are ways to *export* to ONNX format from most common tools, but not many have a way of *importing*.
This makes it very difficult to use for our purposes, as my code would then still be framework dependent, just using an independent way to specify the model.

ONNX does have its own runtime, and I could try and move dense parts of that to the SGX enclave, but that would make comparison harder for the (I assume) not super broadly adopted ONNX runtime.

***** Keras
Keras tries to be a human readable extension sitting on top of Tensorflow, Theano etc.
The layers of Keras (and therefore any custom layers) can be written in pure python, and I think they can then also call C functions.

Storing models in Keras saves the architecture as well as the weights.
Anything that also contains weights is stored as an [[https://en.wikipedia.org/wiki/Hierarchical_Data_Format][HDF5]] file.
It's also possible to store the architecture alone as a json file.

Keras can do rudimentary visualization by using pydot, which uses [[https://github.com/pydot/pydot][pydot]] to interface with [[http://www.graphviz.org/][graphviz]].
The function for this is ~plot_model~ in ~keras.utils~.
Alternatively Keras offers a ~model.summary()~ function which prints a summary of all the layers.

For these reasons, *Keras* seems to be the best choice, offering the versatility and customizability we need along with nice tooling for training, storing and loading, as well as visualization.

**** DONE Design a splitting functionality for Keras [3/3]
     CLOSED: [2019-10-08 Tue 16:05]
RBO wants to have a good way of specifying split position and visualization of it before we try and run the code.
The best way to do this is writing a Keras layer container (similar to Sequential).
This way we have control over how the underlying layers function.

***** DONE Fix plotting for composed Keras models
      CLOSED: [2019-09-25 Wed 15:56]
Keras models can be composed from each other.
There is a concat function, but one model can actually just contain another model.
For both the ~model.summary()~ function and the ~plot~ function however, the contained model is not summarized recursively.
It is just listed as ~sequential_2~.
This will be the first step.

[2019-09-25 Wed 15:55]
~plot_model~ has a parameter for expanding submodules, called ~expand_nested~.
Setting this to true gives output like in [[file:python/mnist_cnn.png][this image]].

***** DONE Finish test code for MNIST in Keras
      CLOSED: [2019-09-25 Wed 16:25]
In order to evaluate I need fully working code for MNIST

***** DONE Write extension to Keras Sequential [5/5]
      CLOSED: [2019-10-08 Tue 16:05]
We want to have a container similar to Keras's ~Sequential~ model that can also generate C code for the contained layers.
It should be possible to iterate over all contained layers and store the ~output_shape~ along the way.
Then I also need to store the ~input_shape~ of the first layer and the weights and biases of all layers.
Getting the activation functions could be a bit trickier.
Here it is probably best to either call the C functions like the Keras names, or introduce a mapping.
I'm not sure right now if I can actually get the name of the activation function or not, which could increase difficulty.

[2019-09-25 Wed 18:32]
When executing a Keras Sequential (e.g. for ~predict()~) at the execution at one point starts calling backend functions.
This is something I can't really trace, and try and emulate this with my extension to ~Sequential~.
Due to this it's smarter to build a function that takes a sequential model and builds the C code from that.
This could then be wrapped in a custom Keras layer that calls the C code.
I would first however have to test whether calling C code from a Keras layer works at all.

[2019-09-26 Thu 10:52]
I took a debugging dive into the ~model.predict()~ function.
It's fairly easy to follow until Keras hands off execution to the Tensorflow backend.
The model is collapsed into a single function, which makes extensive use of the TF session model (ugh...).
This function is then wrapped into a Keras ~Function~ object, which is also a TF ~GraphExecutionFunction~ object when using the TF backend.

****** DONE Test control flow for custom Keras layers
       CLOSED: [2019-09-26 Thu 12:02]
[2019-09-26 Thu 11:54]
Keras only sits on top of whatever backend it uses.
This means that any custom Keras layer really only generates TF tensors.
The contents of their [[https://keras.io/layers/writing-your-own-keras-layers/][~call()~]] function are really only called once, where they are converted to TF Graphexecution functions.
Also, the ~x~ input is a backend tensor, so there is not really any way to call my C function.
For that I would need the actual values, which they don't have at that point, because they are only placeholders...

The real problem is the inherent lazyness of TF 1.14.
*HOWEVER*, there is a way to add eager functions to it, by wrapping them with [[https://www.tensorflow.org/api_docs/python/tf/py_function][~tf.py_func~]].
This is a tensor wrapping a regular C function.
With this I can wrap my ~pymatutil~ function in a regular python function, taking numpy arrays as input.

Keras doesn't really have an equivalent.
I'm currently not sure whether it's smarter to switch to TF in general or just use the TF function in Keras.
I will check how easy it is to get weights out of TF NNs.

With TF 2.0 (which is currently available as a beta) eager execution is the default, so this would make things a lot easier.

[2019-09-26 Thu 16:00]
So I switched to TF 2.0.0 rc2, because the eagerness seems to be exactly what I want.
Unfortunately, during training it still switches to lazy execution, I guess for performance.
However, once everything is done the environment is in eager mode again.
I guess they never meant eager execution to work during full blown execution of the network.
*HOWEVER*, this still helps me to write my code, as I can use the Keras bundled with TF, and use [[https://www.tensorflow.org/api_docs/python/tf/py_function][~tf.py_func~]] to call my code.
Will test tomorrow.

****** DONE Use ~tf.py_func~ to call C code for testing
       CLOSED: [2019-10-01 Tue 09:58]
It should probably be a simple operation like multiplying with 2.

****** DONE Test calling C Code from a custom Keras layer
       CLOSED: [2019-09-26 Thu 11:57]
Does not make sense, see [[*Test control flow for custom Keras layers][this point]], lazyness is super annoying.

[2019-09-27 Fri 09:56]
Actually, with TF as backend this still works (see [[file:python/tf_poc.py::return%20tf.py_function(func=external_func,%20inp=%5Bx%5D,%20Tout=tf.float32)][this code]]).
The input to ~Layer.call()~ is a TF tensor in that case, and I can use that as an input to a ~tf.py_function~.
This ~tf.py_function~ can then call my interop code.

****** DONE Extract weights, biases, etc.
       CLOSED: [2019-10-01 Tue 09:58]
Currently just for Keras, will be done by extending ~Sequential~

****** DONE Generate ~dense()~
       CLOSED: [2019-10-01 Tue 09:58]
Generate the calling code, currently without any template engine.

[2019-10-01 Tue 09:58]
The ~Enclave~ extension of ~Sequential~ has a ~generate_dense()~ function that will build the dense function out of matutil functions.
This is done via format strings, which is not the best solution, but it works for now.

**** DONE Build custom Keras layer for C code execution
     CLOSED: [2019-10-01 Tue 10:00]
After the C code is generated, the custom Sequential model should either call that instead of the underlying layers, or replace itself with a new layer.
That new layer would then either call the Enclave or native C code.

[2019-10-01 Tue 09:59]
This is done by creating a custom Layer that builds a ~tf.py_function~, which calls the python function calling my C interop code.

**** TODO Test on actual hardware                                       :sgx:
We want to get a performance benchmark on actual hardware running my enclave code on the actual enclave.

***** TODO Replicate [[file:related_work/razavian14cnn-transferability.pdf][CNN Features off-the-shelf]]
One paper that Cecilia sent me uses ResNet + an SVM.
Instead we could try and solve the same task using ResNet and some Dense Layers.

****** Dataset
They use the [[http://web.mit.edu/torralba/www/indoor.html][MIT67]], consisting of 67 classes of indoor scenes.
The dataset can be downloaded from [[http://groups.csail.mit.edu/vision/LabelMe/NewImages/indoorCVPR_09.tar][here]], and there are files detailing the [[http://web.mit.edu/torralba/www/TrainImages.txt][training set]] and [[http://web.mit.edu/torralba/www/TestImages.txt][test set]].

Some images need to be converted with command line magic:
#+begin_src bash
find -name "*.jpg" -exec file \-i {} \; | grep -v jpeg | awk -F ':' '{system("convert "$1" "$1)}'
#+end_src
It can be validated that all files are now actually jpegs using the first two commands only:
#+begin_src bash
find -name "*.jpg" -exec file \-i {} \; | grep -v jpeg
#+end_src

***** TODO Find a purely dense model for some task with an existing dataset
Jakob told me to try RNNs.
These are used for language modelling and other use-cases, where there are datasets and models available.

***** TODO Evaluate performance for cutting at different layers
We want a graph that shows relative inference time increase over the number of layers moved into the enclave.

In this graph we could include a measure of "model privacy", ie. ratio of public parameters.
It could also be ratio of private parameters, or private/public parameters.


**** TODO Add capability for convolutional layers :sgx:
**** TODO Test the splitting with several popular architectures :sgx:
We should test the splitting with several common architectures.
RBO also said something about splitting according to some metrics (e.g. GPU memory/utilization), which I'm not too sure about how he meant it.
We could try and split before the first dense layer, I should try and see if there are other layer types used for actual classification.

**** TODO Automate memory layout inside SGX :sgx:
We might have to do some memory magic because otherwise we might run out of memory inside the SGX.
The first prototype can do this explicitly for the chosen network, but for publishing we should do this automatically.

Alternative we could also generate a fixed function from sparse matrix multiplication.
This would mean that we go through the output cell by cell, calculating all immediate steps in a row.
Using this would throw away any shared results, and be much slower.
However, this could help us avoid memory issues.

[2019-09-25 Wed 10:56]
This has to wait until we are testing on actual hardware.
For the simulation mode we are currently running, memory doesn't seem to be an issue.
I'm currently specifying intermediate matrices after every ~fc~ layer of the network (see [[file:lib/sgx/Enclave/Enclave.cpp][the enclave code]]), without any issues.

This either means we are not as memory constrained as we thought, or the simulation simply has a larger amount of memory than the actual enclave would have.

[2019-10-08 Tue 16:09]
The enclave has 100MB of memory.
This might require some trickery for full models.
I could try and use only two temporary buffers, to reduce memory requirements.

**** TODO Anwendungsf√§lle auflisten                                     :sgx:
    [2019-10-04 Fri 11:36]
***** Model stealing
****** DONE Find some papers on model stealing                          :sgx:
       CLOSED: [2019-10-23 Wed 17:20]

***** Offline ML-as-a-service <<Offline MLAAS>>
Making content of ~dense()~ and network weights independent of launch code would allow MLAAS providers to sign an execution environment once, and then give signed and encrypted weights and architectures to their customers.
This would require decrypting the models and dynamically generating the ~dense()~ function.

By doing so we can move MLAAS from the cloud to offline, allowing for easier use, at least on desktop devices.
While this doesn't necessarily work for mobile devices, it would allow users of MLAAS services to run the models themselves.

The resulting classification could even be signed together with the input, thus verifying that the classification came from a certain model.
*This would add accountability to every single classification.*

Maybe we can spin an additional paper from this.

***** Oblivious distributed machine learning
Along the lines of [[Offline MLAAS][this]] and distributed SGD, we can do the same thing the other way round.
Companies could send signed and encrypted inputs, together with model updates to people offering their compute power.
The people then train their local model inside the enclave, so they can't steal the model.
The code in the enclave then signs and encrypts the weight updates and sends them back to the company.

The local devices would all use the same signature if they are provided with the same enclave code, so they wouldn't be identifiable from their updates alone.
This can be cited from [[file:related_work/shokri15privacy.pdf][Shokri ML privacy]], the learning in general would be similar to [[file:related_work/dean2012large.pdf][Downpour]].

This would make an *awesome* paper.

***** Privacy -> Set Membership
I should do the math where the total attack accuracy for only returning the label comes from in the set membership paper.
Other than that we still have the advantage of making an offline model as robust as an online oracle.

****** TODO Leakage confinement
Find out if the leakage information is constrained to the dense layers.
Figure -> dense weights before and after retraining

[2019-10-31 Thu 15:29]
I mean it's contained in the dense layers if you keep it there.
What would be interesting is how much the convolutional layers change if you don't fix them.
Here a delta per individual weight would be interesting.

A comparison of accuracies on the same dataset with and without freezing the convolutional layers would also be beneficial.
This relates to [[Layer freezing in transfer learning][transfer learning]].

****** TODO Run on Cecilias face classifier
Retraining, then try and see where the differences are during retraining.
Evaluate set membership.

***** TODO consider possible new attacks on partial black boxes
An attacker might use the early convolutional layers to reduce the search space for adversarial examples.
This needs to be in the final paper.

***** Lastverteilung zw. Edge, Cloud
****** TODO Find some papers on load balancing for distributed heterogenous architectures :sgx:
Or talk to DPS groups

**** Read related work [5/6]                                  :sgx:
    [2019-10-03 Thu 10:15]

***** DONE Data privacy:
      CLOSED: [2019-10-08 Tue 16:05]
One is [[file:~/Projects/nn-sgx/related_work/shokri15privacy.pdf][on privacy preserving deep learning (leakage)]], which provides a way to perform SGD distributed over multiple nodes.
By not sending all state updates to the state server the participating parties retain privacy of their dataset.
This is cool in the sense that it solves a problem that we could also try and solve with our method (which would give us [[file:related_work/ohrimenko16enclave.pdf][the ohmirenko paper]] exactly).
Other than that there's not too much in common.

The other Shokri paper is [[file:~/Projects/nn-sgx/related_work/shokri17membership.pdf][on membership attacks]].
However, in my opininion this doesn't have too much to do with our case.
The case for k=1 label, however is weird.
Just guessing gives the correct accuracy, but precision and recall are a bit weird.
I should talk to Rainer about this

****** DONE Talk to Rainer about the k=1 label case for set membership attacks
       CLOSED: [2019-10-29 Tue 08:55] SCHEDULED: <2019-10-28 Mon>

***** DONE Enclave Security                                             :sgx:
      CLOSED: [2019-10-21 Mon 10:07]
[[file+sys:related_work/kaptchuk2019state.pdf][This paper]] by Kaptchuk et al. uses ledgers to add state to the enclave, which guarantees that every state is only presented to it once.

We could use this mechanism to implement rate limiting on our models.
This would mean they aren't truly offline anymore, but I guess that's the security/usability tradeoff we have to face.

***** DONE NNs in TEEs
      CLOSED: [2019-10-15 Tue 09:51] SCHEDULED: <2019-10-15 Tue 12:00>
Dan Boneh released [[file:related_work/tramer19slalom.pdf][SLALOM]] with Florian Tramer, talking about something very similar to what we're doing.

[2019-10-15 Tue 10:50]
They do talk about something similar to what we're doing, but they're doing it very differently.
Instead of running parts of the NN in the enclave, they run it on the GPU and verify its outputs in the enclave.
Also they apply some encryption mechanism to the input data to keep it private from the model owner.

The notion of verifying outputs that come from the GPU is very cool, and we should look into something similar for our project.
Encrypting the input might also be a posibility for future work.

[2019-10-26 Sat 19:10]
In 2016 Ohrimenko et al. released a paper on [[file:related_work/ohrimenko16enclave.pdf][training different ML flavors in the enclave]].
Their focus is defending against power analysis attacks to keep the training data private from some third party datacenter.
Not entirely related to our method.

Also what they do requires the entire training process to happen in the enclave, which is a lot slower than training on GPUs, which we could do.

***** DONE [#A] Model stealing
      CLOSED: [2019-10-26 Sat 18:37] SCHEDULED: <2019-10-24 Thu>
A good paper on model stealing seems to be [[file:related_work/tramer16stealing.pdf][this one]] by Tramer et al., which Daniel recommended to me.
The query budget they set for their attack is ~100*k~, where ~k~ is the number of parameters.
The dense part of our model has 18903.045 parameters, so that wouldn't be feasible.

***** TODO Find something on accountability for NNs
[2019-10-26 Sat 17:58]
What I found is more about accountability for [[http://merrill.umd.edu/wp-content/uploads/2016/01/p56-diakopoulos.pdf][automatically generated textual reports]].
A [[https://officialblogofunio.com/2019/04/29/a-short-introduction-to-accountability-in-machine-learning-algorithms-under-the-gdpr/%0A][blog post]] (that is heavily focused on law) talks about accountability of data processing under GDPR.
One could think of a use case where a model is trained, tested for differential privacy and after passing that testing it is approved and signed.
The legislator could then co-sign the code running in the enclave, like a certificate.
Whoever runs the model then can provide proof that the model they are running is differentially private, if that model signs it's outputs.

***** DONE Layer freezing in transfer learning
      CLOSED: [2019-11-04 Mon 18:01] SCHEDULED: <2019-11-04 Mon 18:00>
Cecilia pointed me towards transfer learning.
There should be some papers on this especially in the context of layer freezing.
I should talk to her next week

[2019-11-04 Mon 09:24]
Cecialia sent me an email with resources, I should check them out.

****** TODO Talk to Cecilia about Layer Freezing
After me reading some literature we might be able to have a good discussion.

*** Future Work [0/2]
**** TODO Integrate with the framework API :sgx:
Rainer said it would be nice to integrate the SGX with the tensorflow API (or pytorch, whatever)

**** TODO Examine required query counts for transfer and full model stealing attacks
Transfer attacks require much less precise replications of the target model.
It would be interesting to see how the query numbers differ for actual model stealing and transfer attacks.

** Unforeseen events
*** DONE Figure out how many bits the enclave uses for floats
    CLOSED: [2019-10-04 Fri 10:11]
This could cause some weird results and incompatibilities.
It's also not perfectly clear if the enclave even supports floats.

[2019-10-04 Fri 10:10]
The enclave supports floats.
In all my tests there have been rounding errors (larger than the ~numpy~ default of 10^-8), but the results are identical.
I think this is good enough.

*** DONE Find out what is needed for the foreshadow mitigation          :sgx:
    CLOSED: [2019-10-04 Fri 10:10]
   [2019-10-04 Fri 10:02]
 I should probably read [[https://foreshadowattack.eu/foreshadow.pdf][the paper]].
 According to [[https://foreshadowattack.eu/][the attack's website]] (what a world we live in), under the headline "Are there mitigations against Foreshadow?", mitigations require software and microcode updates.
 I would assume the software updates are for changing keys (which could have been leaked by vulnerable systems) and resealing enclave code.

 The microcode is for patching the vulnerability.
 Intel released a [[https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00161.html][security advisory]] on foreshadow, as well as a summary of microcode updates.
 This means that having currenty microcode updates on our benchmark machines would make them resistant against foreshadow.
 At least as resistant as we can currently get.

*** TODO Test if our processors are vulnerable to foreshadow            :sgx:
    [2019-10-04 Fri 11:46]

If they are, don't update, we want to measure performance impact of that 
Updating is probably ~irreversible~, so we should find out if they are vulnerable before installing any microcode packages.

