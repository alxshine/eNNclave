* NN SGX
Running the dense part of CNNs inside the trusted enclave to reduce leakage and protect against model stealing.
We hope to make this as robust against model stealing as online oracles.

** People
RBO, CPA, ASC

** Project Outline
*** First prototype [14/14]
**** DONE Create working environment for Testing and Development
     CLOSED: [2019-08-20 Tue 19:55]
[2019-08-14 Wed]
The project unfortunately only runs on Ubuntu 18.04 and some other OSes, and not on my own machine...
I'm currently developing inside a VM.

The project currently consists of a minimal Makefile for a single enclave and app that runs in simulation mode.
Adding functions to the enclave requires their definition in the header file, and implementation, as for regular C code.
Additionally, it requires addition to the ~trusted~ block in [[file:Enclave/Enclave.edl::trusted%20{][Enclave/Enclave.edl]], with publicly accessible functions having the ~public~ keyword added to them.

In the app, the functions are then called with a slightly different signature.
The return value of enclave functions is always an sgx_status, and the ~global_eid~ is added to the parameters.
Return values of the function are set by the SGX wrapper through pointers, C-style.

**** DONE Extract Weights from neural net
     CLOSED: [2019-09-16 Mon 16:50]
     I can store the weights of a model in HDF5 format, and then load that using their C++ API.
     Then I could either load the weights from inside the enclave (which gives us no benefit at all), or hardcode them.

     I need the activation functions anyway, so hardcoding is probably the way to go.

     [2019-09-03 Tue 09:40]
     FUCK TENSORFLOW
     The C++ API requires building from source, and that requires bazel, and then everything together is a massive house of cards where the cards randomly self ignite, and then the example doesn't compile and nothing works.
     That seems like an unreasonable amount of super annoying work, which I don't want to do.

     So, instead I will try to create the NN in python, and create cython bindings for my C++ code.
     This probably means I will have to write a wrapper app for the enclave code, and maybe later make calls to the enclave directly.

     [2019-09-04 Wed 14:24]
     Does it have to be Tensorflow?
     PyTorch seems to do the same thing but nicer, because it gives us more granular access to the underlying matrices.

     [2019-09-16 Mon 14:39]
     I moved the code for executing the dense layers to their own function in the ~Net~ class.
     Now I can load the weights from the state_dict and then do the computation using custom built functions for the matrix multiplication.
     This should serve as a good starting point for moving that first to C and then to the SGX enclave.
     
     [2019-09-16 Mon 15:30]
     Finished moving the dense NN calculation to plain numpy.
     It's actually fairly simple, I just need to remember to transpose the weight matrix.
     Here's the source code for it:
#+BEGIN_SRC python
    def dense(self, x):
        state = self.state_dict()
        # breakpoint()
        w1 = state['fc1.weight'].detach().numpy().T
        b1 = state['fc1.bias'].detach().numpy()
        x = x.detach().numpy()
        tmp = np.matmul(x, w1) + b1
        x = np.maximum(tmp, 0)

        w2 = state['fc2.weight'].detach().numpy().T
        b2 = state['fc2.bias'].detach().numpy()
        tmp = np.matmul(x, w2) + b2
        x = np.maximum(tmp, 0)
        return torch.Tensor(x)
#+END_SRC
     
**** DONE Understand biases in Pytorch NNs
     CLOSED: [2019-09-16 Mon 16:50]
I need to understand how biases are used in pytorch, so I can correctly model that behaviour.

     [2019-09-16 Mon 16:50]
They are simply added after the input is multiplied with the weight.
Then the sum of the multiplication result and the biases is run through the nonlinearity.

     [2019-09-25 Wed 10:54]
The reason why I was so confused by biases in Pytorch is the fact that multiple inputs in a batch are ~hstacked~ together into a single matrix.
This means the computation the tensor does internally is more than simple matrix multiplication.
In order to do this correctly manually, one has to iterate over every column and compute the matrix multiplication for that column alone.
Then the dimensions of the bias vector also fit again.

**** DONE Write a script that generates C arrays from pytorch weight matrices
     CLOSED: [2019-09-16 Mon 16:51]
     Because the weights should be hardcoded into the enclave (this avoids decryption during loading for now), I need a script to extract weights from a ~.pt~ file and generate C array definitions from it.

     See [[./python/torch/gen_headers.py][this script]].

**** DONE Choose test network for first prototype
     CLOSED: [2019-09-05 Thu 10:54]
     Just use MNIST CNN, I have working code for torch

**** DONE Write naive matrix multiplication code in C [3/3]
     CLOSED: [2019-09-18 Wed 10:13]
Just simple stuff, including testing
- [X] matrix matrix multiplication
- [X] matrix matrix addition
- [X] nonlinearity

**** DONE Compare results of torch, numpy and my C code
     CLOSED: [2019-09-19 Thu 10:29]
There seem to be some differences in results that can't entirely be blamed on rounding errors.
The differences are currently in the range of e-7, which is more than numpy uses for its ~np.allclose~ function (e-8).
I will build a small network and then compare all three methods of computation, to see what is happening and where it starts to diverge.

[2019-09-19 Thu 10:26]
Finished evaluation, see [[file:test_correctness.py::fc%20=%20nn.Linear(IN_FEATURES,%20OUT_FEATURES,%20bias=False)][test_correctness.py]].
Even though the differences are around e-6, they seem to be normal distributed, and the max/min increases with increased matrix size (which strengthens my belief in the normal distribution).
I don't think it is a problem, and if it becomes one I already have a test script set up for evaluation.

**** DONE Move the fully connected layers to naive matrix multiplication in C
     CLOSED: [2019-09-19 Thu 10:29]
In the forward function of the network, instead of invoking ~nn.Linear~ I can call a C function.
This means that pytorch doesn't know how to backpropagate, but it doesn't need to anymore.

[2019-09-18 Wed 17:10]
The numpy and C variants do give slightly different results than the pure pytorch variant (in the e-7 range).
Jakob thinks this is more than just rounding errors, so I should check that out.
See [[*Compare results of torch, numpy and my C code][this TODO]]

**** DONE Combine C functions into one ~dense~ function
     CLOSED: [2019-09-19 Thu 12:06]
     This function can then be moved to the enclave, otherwise it leaks intermediate values

**** DONE Add compile_commands
     CLOSED: [2019-09-19 Thu 14:21]
Added a single recursive Makefile, which can be wrapped using bear

**** DONE Check the details of parameter passing into enclaves
     CLOSED: [2019-09-23 Mon 11:34]
Simply passing input pointers to the matrices doesn't work, so I will need to test this with some smaller examples
[2019-09-23 Mon 11:33]
Adding a [in, count=s] parameter for every array tells the SGX autogenerated wrapper how large the array should be, and it is then copied to enclave memory

**** DONE Do naive matrix multiplication inside the SGX enclave
     CLOSED: [2019-09-23 Mon 11:34]
Move the aforementioned code function to the enclave.
With a well enough defined interface this shouldn't be too much work

**** DONE Feed result back into pytorch, or calculate softmax
     CLOSED: [2019-09-19 Thu 14:25]
As we want this to be an oracle, I should execute the "softmax" (just taking the maximum)

[2019-09-19 Thu 14:21]
matutil_dense() already only returns the label

**** DONE Find a way to actually call the Enclave from PyTorch
     CLOSED: [2019-09-23 Mon 14:55]
Currently the enclave is not called from pytorch
The wrapper is called correctly, and then no error happens, but the enclave is not called.

[2019-09-23 Mon 14:22]
I had not initialized the enclave, and did not really output any errors in the enclave wrapper.
So yeah, this one's on me.

[2019-09-23 Mon 14:40]
Added some error handling to pymatutil.

*** Improvements for paper [3/9]
**** TODO Test on actual hardware [3/5]                                 :sgx:
We want to test this on some actual hardware.
For a representative evaluation we should use one consumer grade, and one server grade CPU.
According to [[https://software.intel.com/en-us/forums/intel-software-guard-extensions-intel-sgx/topic/606636][this forum answer]] Intel SGX was introduced with Skylake generation CPUs, so any 6xxx processor will be fine.

***** DONE Write an e-mail to Manuel
      CLOSED: [2019-09-25 Wed 13:31]
***** DONE Set up the machine
      CLOSED: [2019-10-08 Tue 15:25]
I have a [[file:setup_sgx_machine.sh][setup script]] for Ubuntu 19.04, which I tested on VMs.

[2019-10-04 Fri 12:00]
The hardware is set up, it only needs a setup of Ubuntu, and network access.
***** DONE Plan evaluation
      CLOSED: [2019-10-11 Fri 11:32]
****** DONE Coordinate with RBO how exactly we evaluate                 :sgx:
       CLOSED: [2019-10-08 Tue 17:29]
I work out the exact benchmarking details with him.
He said he wanted to test different cutting points, so I should also add Convolutional layers to the enclave.

Convolutional layers in the front.
Ideally more than two dense layers in the end.
We can look at ImageNet structures, which should have more dense layers.

***** TODO Perform actual evaluation

****** WATING Get VGG Face Dataset up and running
This is large enough to merit its own project, and it's currently underway

****** DONE Prepare evaluation using TF built-in datasets
       CLOSED: [2019-10-15 Tue 11:54] SCHEDULED: <2019-10-15 Tue 12:00>
TF has a built in, pretrained version of VGG16.
It also has multiple built-in datasets which can be used for retraining the dense layers.
This can be used as a preparation for the evaluation.

[2019-10-15 Tue 11:41]
The tensorflow_datasets library is a bit weird to use, so for now I'm sticking to a dataset that's downloaded from the tensorflow servers.

[2019-10-15 Tue 11:48]
Added evaluation scripts for both versions of vgg_flowers.
Now only the timing needs to be set up.

****** TODO Think about good ways to measure inference time
We have one-time setups that take place, which need to be measured.
There is also additional communication between the GPU and CPU that needs to take place.

We should measure:
- single inference time, which should include enclave setup
- time for a batch predict, so we have a better understanding of how the enclave performs over larger datasets
- check if the entire model fits in the 128MB that we have

****** DONE Convert trained VGG flowers model to enclave                :sgx:
       CLOSED: [2019-10-16 Wed 15:30] SCHEDULED: <2019-10-16 Wed>
[2019-10-15 Tue 15:35]
The original model with two layers of 4096 neurons is too large to reasonably convert using my current toolchain.
They are also too large to fit in the enclave without paging.
I reduced the number of neurons in these layers to 2048 and requeued the training, it's currently enqueued.

[2019-10-15 Tue 17:14]
I'm currently running into the problem that the weight matrices are just too large.
I split the first weight matrix into its own file, and tried to compile only that, but the compilation took up all 16GB of RAM available on my machine.
The next step would be to look into building ELF files by hand, or simply switching to a more powerful machine.

[2019-10-16 Wed 09:31]
I'm currently looking into what the problem with the state is.
My intuition says that it's even before the actual compilation.
To verify this, I'm using clang with the ~-emit-ast~ option.

If this goes through, then it could be the constant table that's being built during compilation.
I might be able to turn this off during compilation, but I'm not sure.

[2019-10-16 Wed 15:28]
I changed the output so it generates binary files containing the weight matrices.
The code now compiles without issues.
However, it's still too large for the enclave

****** TODO Resolve the problem of too large networks
The network as it is doesn't fit in the enclave.
I should first look at the ~GlobalAveragingLayer~ (or similar) in TF.
This might give us the much needed size reduction.

[2019-10-16 Wed 15:50]
I just checked the included VGG16 model, whose structure looks like the following:
#+BEGIN_SRC python :results output
  from tensorflow.keras.applications import VGG16
  VGG16().summary()
#+END_SRC

#+RESULTS:
#+begin_example
Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 224, 224, 3)]     0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544 
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
predictions (Dense)          (None, 1000)              4097000   
=================================================================
Total params: 138,357,544
Trainable params: 138,357,544
Non-trainable params: 0
_________________________________________________________________
#+end_example

As you can see, there are 25088 inputs to the dense part of the network.
This does require some additional reduction before I can use it.

******* TODO try an additional pooling layer
As a simple first step I will try adding an additional pooling layer before the dense part of the network.

******* TODO try an ~GlobalAveragePooling2D~ in vgg_flowers code
As a hopefully better measure I will try to add a global average pooling layer before the flatten.
This will greatly reduce dimensionality, and seems to be a SOTA technique.
Hopefully tests from the flower dataset generalize to the face dataset.

****** TODO Finish performance evaluation
enclave vs. GPU for multiple cutoff points
Test with enclave and with native C code

MNIST, one more type of data
******* DONE get VGG16 model
	CLOSED: [2019-10-15 Tue 08:53]
There is one included within ~keras.applications~.
I think we should use it, with standard TF datasets.

[2019-10-15 Tue 08:53]
We are probably using that network, with the pre-trained weights.
However the end goal will be to evaluate it on the VGG face datset, when that is available to us.

****** TODO Plan privacy evaluation
more on this later

**** DONE Find out how model architectures are stored
     CLOSED: [2019-09-25 Wed 10:49]
We want to be able to specify a cut similar to current SOTA architecture visualization tools.
***** PyTorch
PyTorch uses the python classes for model definition, usually only storing the state_dict.
However, it is also possible to store the entire *module* where the model is defined, using Python's [[https://docs.python.org/3/library/pickle.html][pickle]] module.
Internally this does the same thing as keeping the file containing the python class and loading the state_dict, as it stores that file as well.
This means it is dependent on the containing folder structure, which might lead to some very weird errors.

PyTorch has no innate visualization tool, it instead is directed more at a programmer's view of things, as it allows for debugging the actual code of the ~forward~ function.
What one can do is export the model to [[ONNX][ONNX]] format, and then visualize it using something like [[https://github.com/PaddlePaddle/VisualDL][VisualDL]].

For our separation we could create some macro, wrapper, whatever that is then easily plugged into the definition of ~forward~.

***** Tensorflow
For Tensorflow, everything is in the session.
The graph that describes the model, the state of layers, optimizers etc.
The problem is that this way I'm not sure if my code would even work, as this would mean rebuilding everything so it works with Tensorflow.

Tensorflow visualizes its graphs using Tensorboard, and gets the info for that from the log directory.
A link to this is [[https://www.tensorflow.org/guide/graph_viz][here]].
I'm not sure how easy we can use Tensorboard for input, as Tensorflow in general is hard to edit.

***** Open Neural Network Exchange - <<ONNX>>
[[https://github.com/onnx/onnx][ONNX]] is meant to be a framework independent way of specifying models, complete with data types and operators.
There are ways to *export* to ONNX format from most common tools, but not many have a way of *importing*.
This makes it very difficult to use for our purposes, as my code would then still be framework dependent, just using an independent way to specify the model.

ONNX does have its own runtime, and I could try and move dense parts of that to the SGX enclave, but that would make comparison harder for the (I assume) not super broadly adopted ONNX runtime.

***** Keras
Keras tries to be a human readable extension sitting on top of Tensorflow, Theano etc.
The layers of Keras (and therefore any custom layers) can be written in pure python, and I think they can then also call C functions.

Storing models in Keras saves the architecture as well as the weights.
Anything that also contains weights is stored as an [[https://en.wikipedia.org/wiki/Hierarchical_Data_Format][HDF5]] file.
It's also possible to store the architecture alone as a json file.

Keras can do rudimentary visualization by using pydot, which uses [[https://github.com/pydot/pydot][pydot]] to interface with [[http://www.graphviz.org/][graphviz]].
The function for this is ~plot_model~ in ~keras.utils~.
Alternatively Keras offers a ~model.summary()~ function which prints a summary of all the layers.

For these reasons, *Keras* seems to be the best choice, offering the versatility and customizability we need along with nice tooling for training, storing and loading, as well as visualization.

**** DONE Design a splitting functionality for Keras [3/3]
     CLOSED: [2019-10-08 Tue 16:05]
RBO wants to have a good way of specifying split position and visualization of it before we try and run the code.
The best way to do this is writing a Keras layer container (similar to Sequential).
This way we have control over how the underlying layers function.

***** DONE Fix plotting for composed Keras models
      CLOSED: [2019-09-25 Wed 15:56]
Keras models can be composed from each other.
There is a concat function, but one model can actually just contain another model.
For both the ~model.summary()~ function and the ~plot~ function however, the contained model is not summarized recursively.
It is just listed as ~sequential_2~.
This will be the first step.

[2019-09-25 Wed 15:55]
~plot_model~ has a parameter for expanding submodules, called ~expand_nested~.
Setting this to true gives output like in [[file:python/mnist_cnn.png][this image]].

***** DONE Finish test code for MNIST in Keras
      CLOSED: [2019-09-25 Wed 16:25]
In order to evaluate I need fully working code for MNIST

***** DONE Write extension to Keras Sequential [5/5]
      CLOSED: [2019-10-08 Tue 16:05]
We want to have a container similar to Keras's ~Sequential~ model that can also generate C code for the contained layers.
It should be possible to iterate over all contained layers and store the ~output_shape~ along the way.
Then I also need to store the ~input_shape~ of the first layer and the weights and biases of all layers.
Getting the activation functions could be a bit trickier.
Here it is probably best to either call the C functions like the Keras names, or introduce a mapping.
I'm not sure right now if I can actually get the name of the activation function or not, which could increase difficulty.

[2019-09-25 Wed 18:32]
When executing a Keras Sequential (e.g. for ~predict()~) at the execution at one point starts calling backend functions.
This is something I can't really trace, and try and emulate this with my extension to ~Sequential~.
Due to this it's smarter to build a function that takes a sequential model and builds the C code from that.
This could then be wrapped in a custom Keras layer that calls the C code.
I would first however have to test whether calling C code from a Keras layer works at all.

[2019-09-26 Thu 10:52]
I took a debugging dive into the ~model.predict()~ function.
It's fairly easy to follow until Keras hands off execution to the Tensorflow backend.
The model is collapsed into a single function, which makes extensive use of the TF session model (ugh...).
This function is then wrapped into a Keras ~Function~ object, which is also a TF ~GraphExecutionFunction~ object when using the TF backend.

****** DONE Test control flow for custom Keras layers
       CLOSED: [2019-09-26 Thu 12:02]
[2019-09-26 Thu 11:54]
Keras only sits on top of whatever backend it uses.
This means that any custom Keras layer really only generates TF tensors.
The contents of their [[https://keras.io/layers/writing-your-own-keras-layers/][~call()~]] function are really only called once, where they are converted to TF Graphexecution functions.
Also, the ~x~ input is a backend tensor, so there is not really any way to call my C function.
For that I would need the actual values, which they don't have at that point, because they are only placeholders...

The real problem is the inherent lazyness of TF 1.14.
*HOWEVER*, there is a way to add eager functions to it, by wrapping them with [[https://www.tensorflow.org/api_docs/python/tf/py_function][~tf.py_func~]].
This is a tensor wrapping a regular C function.
With this I can wrap my ~pymatutil~ function in a regular python function, taking numpy arrays as input.

Keras doesn't really have an equivalent.
I'm currently not sure whether it's smarter to switch to TF in general or just use the TF function in Keras.
I will check how easy it is to get weights out of TF NNs.

With TF 2.0 (which is currently available as a beta) eager execution is the default, so this would make things a lot easier.

[2019-09-26 Thu 16:00]
So I switched to TF 2.0.0 rc2, because the eagerness seems to be exactly what I want.
Unfortunately, during training it still switches to lazy execution, I guess for performance.
However, once everything is done the environment is in eager mode again.
I guess they never meant eager execution to work during full blown execution of the network.
*HOWEVER*, this still helps me to write my code, as I can use the Keras bundled with TF, and use [[https://www.tensorflow.org/api_docs/python/tf/py_function][~tf.py_func~]] to call my code.
Will test tomorrow.

****** DONE Use ~tf.py_func~ to call C code for testing
       CLOSED: [2019-10-01 Tue 09:58]
It should probably be a simple operation like multiplying with 2.

****** DONE Test calling C Code from a custom Keras layer
       CLOSED: [2019-09-26 Thu 11:57]
Does not make sense, see [[*Test control flow for custom Keras layers][this point]], lazyness is super annoying.

[2019-09-27 Fri 09:56]
Actually, with TF as backend this still works (see [[file:python/tf_poc.py::return%20tf.py_function(func=external_func,%20inp=%5Bx%5D,%20Tout=tf.float32)][this code]]).
The input to ~Layer.call()~ is a TF tensor in that case, and I can use that as an input to a ~tf.py_function~.
This ~tf.py_function~ can then call my interop code.

****** DONE Extract weights, biases, etc.
       CLOSED: [2019-10-01 Tue 09:58]
Currently just for Keras, will be done by extending ~Sequential~

****** DONE Generate ~dense()~
       CLOSED: [2019-10-01 Tue 09:58]
Generate the calling code, currently without any template engine.

[2019-10-01 Tue 09:58]
The ~Enclave~ extension of ~Sequential~ has a ~generate_dense()~ function that will build the dense function out of matutil functions.
This is done via format strings, which is not the best solution, but it works for now.

**** DONE Build custom Keras layer for C code execution
     CLOSED: [2019-10-01 Tue 10:00]
After the C code is generated, the custom Sequential model should either call that instead of the underlying layers, or replace itself with a new layer.
That new layer would then either call the Enclave or native C code.

[2019-10-01 Tue 09:59]
This is done by creating a custom Layer that builds a ~tf.py_function~, which calls the python function calling my C interop code.

**** TODO Add capability for convolutional layers :sgx:
     
**** TODO Test the splitting with several popular architectures :sgx:
We should test the splitting with several common architectures.
RBO also said something about splitting according to some metrics (e.g. GPU memory/utilization), which I'm not too sure about how he meant it.
We could try and split before the first dense layer, I should try and see if there are other layer types used for actual classification.

**** TODO Automate memory layout inside SGX :sgx:
We might have to do some memory magic because otherwise we might run out of memory inside the SGX.
The first prototype can do this explicitly for the chosen network, but for publishing we should do this automatically.

Alternative we could also generate a fixed function from sparse matrix multiplication.
This would mean that we go through the output cell by cell, calculating all immediate steps in a row.
Using this would throw away any shared results, and be much slower.
However, this could help us avoid memory issues.

[2019-09-25 Wed 10:56]
This has to wait until we are testing on actual hardware.
For the simulation mode we are currently running, memory doesn't seem to be an issue.
I'm currently specifying intermediate matrices after every ~fc~ layer of the network (see [[file:lib/sgx/Enclave/Enclave.cpp][the enclave code]]), without any issues.

This either means we are not as memory constrained as we thought, or the simulation simply has a larger amount of memory than the actual enclave would have.

[2019-10-08 Tue 16:09]
The enclave has 100MB of memory.
This might require some trickery for full models.
I could try and use only two temporary buffers, to reduce memory requirements.

**** TODO Anwendungsf√§lle auflisten                                     :sgx:
    [2019-10-04 Fri 11:36]
***** Model stealing
****** TODO Find some papers on model stealing :sgx:

***** Offline ML-as-a-service <<Offline MLAAS>>
Making content of ~dense()~ and network weights independent of launch code would allow MLAAS providers to sign an execution environment once, and then give signed and encrypted weights and architectures to their customers.
This would require decrypting the models and dynamically generating the ~dense()~ function.

By doing so we can move MLAAS from the cloud to offline, allowing for easier use, at least on desktop devices.
While this doesn't necessarily work for mobile devices, it would allow users of MLAAS services to run the models themselves.

The resulting classification could even be signed together with the input, thus verifying that the classification came from a certain model.
*This would add accountability to every single classification.*

Maybe we can spin an additional paper from this.

***** Oblivious distributed machine learning
Along the lines of [[Offline MLAAS][this]] and distributed SGD, we can do the same thing the other way round.
Companies could send signed and encrypted inputs, together with model updates to people offering their compute power.
The people then train their local model inside the enclave, so they can't steal the model.
The code in the enclave then signs and encrypts the weight updates and sends them back to the company.

The local devices would all use the same signature if they are provided with the same enclave code, so they wouldn't be identifiable from their updates alone.
This can be cited from [[file:related_work/shokri15privacy.pdf][Shokri ML privacy]], the learning in general would be similar to [[file:related_work/dean2012large.pdf][Downpour]].

This would make an *awesome* paper.

***** Privacy -> Set Membership
I should do the math where the total attack accuracy for only returning the label comes from in the set membership paper.
Other than that we still have the advantage of making an offline model as robust as an online oracle.

****** TODO Leakage confinement
Find out if the leakage information is constrained to the dense layers.
Figure -> dense weights before and after retraining

****** TODO Run on Cecilias face classifier
Retraining, then try and see where the differences are during retraining.
Evaluate set membership.

***** TODO consider possible new attacks on partial black boxes
An attacker might use the early convolutional layers to reduce the search space for adversarial examples.
This needs to be in the final paper.

***** Lastverteilung zw. Edge, Cloud
****** TODO Find some papers on load balancing for distributed heterogenous architectures :sgx:
Or talk to DPS groups

**** TODO [#A] Read related work [2/3]                                  :sgx:
    [2019-10-03 Thu 10:15]

***** DONE Privacy for ML:
      CLOSED: [2019-10-08 Tue 16:05]
One is [[file:~/Projects/nn-sgx/related_work/shokri15privacy.pdf][on privacy preserving deep learning (leakage)]] and the other is [[file:~/Projects/nn-sgx/related_work/shokri17membership.pdf][on membership attacks]].

***** TODO Security in Enclave: :sgx:
[[file+sys:related_work/kaptchuk2019state.pdf][This paper]] uses ledgers to add state to the enclave, which guarantees that every state is only presented to it ones.

***** DONE NNs in TEEs
      CLOSED: [2019-10-15 Tue 09:51] SCHEDULED: <2019-10-15 Tue 12:00>
Dan Boneh released [[file:related_work/Tramer2019_TEE_DeepNetworkOutsourcing_ICLR.pdf][this paper]], talking about something very similar to what we're doing.
[2019-10-15 Tue 10:50]
They do talk about something similar to what we're doing, but they're doing it very differently.
Also they are trying to protect inputs from the model owner, not the model itself.

The notion of verifying outputs that come from the GPU is very cool, and we should look into something similar for our project.

*** Future Work [0/1]
**** TODO Integrate with the framework API :sgx:
Rainer said it would be nice to integrate the SGX with the tensorflow API (or pytorch, whatever)
** Unforeseen events
*** DONE Figure out how many bits the enclave uses for floats
    CLOSED: [2019-10-04 Fri 10:11]
This could cause some weird results and incompatibilities.
It's also not perfectly clear if the enclave even supports floats.

[2019-10-04 Fri 10:10]
The enclave supports floats.
In all my tests there have been rounding errors (larger than the ~numpy~ default of 10^-8), but the results are identical.
I think this is good enough.

*** DONE Find out what is needed for the foreshadow mitigation          :sgx:
    CLOSED: [2019-10-04 Fri 10:10]
   [2019-10-04 Fri 10:02]
 I should probably read [[https://foreshadowattack.eu/foreshadow.pdf][the paper]].
 According to [[https://foreshadowattack.eu/][the attack's website]] (what a world we live in), under the headline "Are there mitigations against Foreshadow?", mitigations require software and microcode updates.
 I would assume the software updates are for changing keys (which could have been leaked by vulnerable systems) and resealing enclave code.

 The microcode is for patching the vulnerability.
 Intel released a [[https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00161.html][security advisory]] on foreshadow, as well as a summary of microcode updates.
 This means that having currenty microcode updates on our benchmark machines would make them resistant against foreshadow.
 At least as resistant as we can currently get.

*** TODO Test if our processors are vulnerable to foreshadow            :sgx:
    [2019-10-04 Fri 11:46]

If they are, don't update, we want to measure performance impact of that 
Updating is probably ~irreversible~, so we should find out if they are vulnerable before installing any microcode packages.
